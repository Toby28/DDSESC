{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOKE6aLfaoOy7bfQRM3AC7z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"44c990cc35204d1495e18de22d9f5fed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74403c8d77814b8599734836929d9ec7","IPY_MODEL_06577dd2ec114452822c584c7040c3fd","IPY_MODEL_7bcf00c239ed461a913b6c083a7ab713"],"layout":"IPY_MODEL_efac9ca6c88a4922a1994967957d14d7"}},"74403c8d77814b8599734836929d9ec7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f09be6ce50cf417fada2005594757fda","placeholder":"​","style":"IPY_MODEL_976222d5965c468b9d0bd6993c1b8315","value":"100%"}},"06577dd2ec114452822c584c7040c3fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f3071c37794e36875edbc1f9df8bee","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_760f5a9124584f74abbf0ed758749c24","value":400}},"7bcf00c239ed461a913b6c083a7ab713":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce06a0ce70e24183a1cd88dcbbe98c8a","placeholder":"​","style":"IPY_MODEL_987085b9d90e43af83669a271272c831","value":" 400/400 [02:47&lt;00:00,  2.38it/s]"}},"efac9ca6c88a4922a1994967957d14d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f09be6ce50cf417fada2005594757fda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"976222d5965c468b9d0bd6993c1b8315":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44f3071c37794e36875edbc1f9df8bee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"760f5a9124584f74abbf0ed758749c24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce06a0ce70e24183a1cd88dcbbe98c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"987085b9d90e43af83669a271272c831":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a00a882bcaf34443af9083df69b222bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_933edece347d499bb3b4ff26f6d38c86","IPY_MODEL_9b406b2416fa40fe838597ed66dd624d","IPY_MODEL_dd89de1385a74ab18b4bfc089a698168"],"layout":"IPY_MODEL_903e326d2f474df8b329a00a24e17bb2"}},"933edece347d499bb3b4ff26f6d38c86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e6a1f1bb14d46e2be0eb69c12ddc505","placeholder":"​","style":"IPY_MODEL_32a20dc32bba483aa1f8afb29a89acf9","value":"100%"}},"9b406b2416fa40fe838597ed66dd624d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f9e0dcfb41e4a01b33196288c981335","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71c674551a6a469a8f9a62f3907cf425","value":400}},"dd89de1385a74ab18b4bfc089a698168":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e916ced43834fc083ba3d10e853151d","placeholder":"​","style":"IPY_MODEL_5cf88d6c79e844bba6da07c56bf1f42b","value":" 400/400 [03:22&lt;00:00,  1.95it/s]"}},"903e326d2f474df8b329a00a24e17bb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e6a1f1bb14d46e2be0eb69c12ddc505":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32a20dc32bba483aa1f8afb29a89acf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f9e0dcfb41e4a01b33196288c981335":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71c674551a6a469a8f9a62f3907cf425":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e916ced43834fc083ba3d10e853151d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cf88d6c79e844bba6da07c56bf1f42b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fa697850dee445ebf24992ba9cc46e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c21f8e2773a4a7fb7f2fbbfebb8f907","IPY_MODEL_fa4643b78c32461d83b0eb1bd836d197","IPY_MODEL_072399c78b5145eeab04ac5ba8521b85"],"layout":"IPY_MODEL_580971feadda498a9c70e1c2556d78d8"}},"7c21f8e2773a4a7fb7f2fbbfebb8f907":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c4b75a4fd014cf4881ba0382b09d0da","placeholder":"​","style":"IPY_MODEL_d9d5c98fd23147a49de448c2c498a110","value":"100%"}},"fa4643b78c32461d83b0eb1bd836d197":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d460975df7224d5cb79260775f47d883","max":400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef7ce8fe162e4622ab85a087571aeda5","value":400}},"072399c78b5145eeab04ac5ba8521b85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73018042acc24943bc56c4fdfbf48e3a","placeholder":"​","style":"IPY_MODEL_47306c6512384726bfb9e97d3f47eb7e","value":" 400/400 [15:30&lt;00:00,  2.28s/it]"}},"580971feadda498a9c70e1c2556d78d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c4b75a4fd014cf4881ba0382b09d0da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9d5c98fd23147a49de448c2c498a110":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d460975df7224d5cb79260775f47d883":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef7ce8fe162e4622ab85a087571aeda5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73018042acc24943bc56c4fdfbf48e3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47306c6512384726bfb9e97d3f47eb7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJZj-VAjDrP8","executionInfo":{"status":"ok","timestamp":1738868041171,"user_tz":300,"elapsed":21325,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"adb6b734-5596-4c43-e3d3-aa6ebdee3a49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","import torch\n","\n","list_dir=['Angry','Neutral','Surprise','Sad','Happy']\n","emo_idx={'Angry':0,'Neutral':1,'Surprise':2,'Sad':3,'Happy': 4}\n","\n","def get_feature():\n","  flag=0\n","  features=0\n","  spklabels=0\n","  emolabels=0\n","\n","  for i in range(11,21):\n","      #input_address='C:/Users/64659/Downloads/Emotional Speech Dataset (ESD)/ESD/'+str(i)+'/'\n","      # output_address='/content/gdrive/MyDrive/paper4_dataset/ESD_devector/'+str(i)+'/'\n","      # output_address='/content/gdrive/MyDrive/paper4_dataset/ESD_resnet/'+str(i)+'/'\n","      output_address='/content/gdrive/MyDrive/paper4_dataset/ESD_ecapa/'+str(i)+'/'\n","      for j in list_dir:\n","          output_dir_tp=output_address + j +'/'\n","          tp_emo=torch.tensor([emo_idx[j]], dtype=torch.int64)\n","          tp_emo=tp_emo.unsqueeze(0)\n","\n","          for file in os.listdir(output_dir_tp):\n","            #if num>10: break\n","            tp_input_address=output_dir_tp + file\n","\n","            tp=torch.load(tp_input_address,map_location=torch.device('cpu'))\n","            tp=tp.unsqueeze(0)\n","\n","            tp_spk = torch.tensor([int(i-11)], dtype=torch.int64)\n","            tp_spk=tp_spk.unsqueeze(0)\n","            #print(tp.shape)\n","            #print(tp2.shape)\n","\n","            if flag==0:\n","              flag=1\n","              features=tp\n","              spklabels=tp_spk\n","              emolabels=tp_emo\n","            else:\n","                features=torch.cat((features,tp), axis=0)\n","                spklabels=torch.cat((spklabels,tp_spk), axis=0)\n","                emolabels=torch.cat((emolabels,tp_emo), axis=0)\n","  return features, emolabels,spklabels\n","\n","feats,emolabels,spklabels=get_feature()\n","print(feats.shape)\n","print(emolabels.shape)\n","print(spklabels.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lwof1s1gD1ab","executionInfo":{"status":"ok","timestamp":1738874238634,"user_tz":300,"elapsed":76029,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"550e1087-77c2-4be4-aea2-7d3c10569311"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-40-32c29afa58f5>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  tp=torch.load(tp_input_address,map_location=torch.device('cpu'))\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([17501, 192])\n","torch.Size([17501, 1])\n","torch.Size([17501, 1])\n"]}]},{"cell_type":"code","source":["data=torch.cat((feats,emolabels,spklabels),axis=1)\n","print(data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AD_4ueXVED4v","executionInfo":{"status":"ok","timestamp":1738874776640,"user_tz":300,"elapsed":329,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"afae977f-87bd-4f3a-cc0c-cf69e99841c2"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([17501, 194])\n"]}]},{"cell_type":"code","source":["print(data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nyn2zB75Q14E","executionInfo":{"status":"ok","timestamp":1738869101757,"user_tz":300,"elapsed":344,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"0796bf27-74e9-472b-a705-9d5378d0aa3e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([17501, 194])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Subset\n","from sklearn.model_selection import train_test_split\n","\n","TEST_SIZE = 0.1\n","BATCH_SIZE = 32\n","SEED = 42\n","\n","# generate indices: instead of the actual data we pass in integers instead\n","train_indices, test_indices, _, _ = train_test_split(\n","    range(len(data.detach().numpy())),\n","    data.detach().numpy()[:,-1],\n","    stratify=data.detach().numpy()[:,-1],\n","    test_size=TEST_SIZE,\n","    random_state=SEED\n",")\n","\n","# generate subset based on indices\n","train_split = Subset(data, train_indices)\n","test_split = Subset(data, test_indices)\n","\n","# create batches\n","train_dataloader_simple = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataloader_simple = DataLoader(test_split, batch_size=BATCH_SIZE)"],"metadata":{"id":"zaLmlUGHQ16m","executionInfo":{"status":"ok","timestamp":1738874779014,"user_tz":300,"elapsed":308,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Pil1cwpDQ19E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","class Network_VAE(nn.Module):\n","    def __init__(self, input_dim=256, output_dim=256, num_domains=2, hidden_dim=384,emoclassnum=5,spkclassnum=10):\n","        super().__init__()\n","        self.output_dim=output_dim\n","        layers = []\n","        layers += [nn.Linear(input_dim, hidden_dim)]\n","        layers += [nn.ReLU()]\n","        for _ in range(2):\n","            layers += [nn.Linear(hidden_dim, hidden_dim)]\n","            layers += [nn.LayerNorm(hidden_dim)]\n","        layers +=[nn.Linear(hidden_dim, output_dim * 2)]\n","        layers +=[nn.LayerNorm(output_dim * 2)]\n","\n","\n","        self.shared = nn.Sequential(*layers)\n","\n","        # self.speakerencoder=nn.Sequential(*[nn.Linear(hidden_dim, hidden_dim),\n","        #                                     nn.LayerNorm(hidden_dim),\n","        #                                     nn.Linear(hidden_dim, output_dim * 2),\n","        #                                     nn.LayerNorm(hidden_dim)])\n","\n","        # self.emotionencoder=nn.Sequential(*[nn.Linear(hidden_dim, hidden_dim),\n","        #                                     nn.LayerNorm(hidden_dim),\n","        #                                     nn.Linear(hidden_dim, output_dim * 2),\n","        #                                     nn.LayerNorm(hidden_dim)])\n","\n","        self.decoder=nn.Sequential(*[nn.Linear(output_dim, hidden_dim),\n","                                      nn.ReLU(),\n","                                      nn.Linear(hidden_dim, input_dim)]\n","                                   )\n","\n","        # self.emocls = nn.Sequential(*[nn.Linear(output_dim * 2, emoclassnum)])\n","\n","        # self.spkcls = nn.Sequential(*[nn.Linear(output_dim * 2, spkclassnum)])\n","\n","\n","\n","        '''\n","        self.unshared = nn.ModuleList()\n","        for _ in range(num_domains):\n","            self.unshared += [nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n","                                            nn.ReLU(),\n","                                            nn.Linear(hidden_dim, hidden_dim),\n","                                            nn.ReLU(),\n","                                            nn.Linear(hidden_dim, style_dim))]\n","        '''\n","    def forward(self, x):\n","        h=self.shared(x)\n","\n","        # emo_mu_logstd=self.emotionencoder(h)\n","        # spk_mu_logstd=self.speakerencoder(h)\n","\n","        # emo_mu, emo_log_std = torch.split(emo_mu_logstd,self.output_dim, dim=1)\n","        # spk_mu, spk_log_std = torch.split(spk_mu_logstd,self.output_dim, dim=1)\n","\n","        mu, log_std = torch.split(h,self.output_dim, dim=1)\n","\n","        z = sample(mu, log_std)\n","\n","        x_hat = self.decoder(z)\n","\n","        return {'x_hat': x_hat, 'mu': mu, 'log_std': log_std,'z': z}\n","\n","\n","    def get_feature(self,x):\n","        h=self.shared(x)\n","\n","        # spk_mu_logstd=self.speakerencoder(h)\n","\n","        return h\n","    '''\n","    def get_emo(self,x):\n","        h=self.shared(x)\n","        emo_mu, emo_log_std = self.emotionencoder(h)\n","        dec_in = torch.cat([emo_mu, emo_log_std], dim=1)\n","        emoresult=self.emocls(dec_in)\n","\n","    def get_spk(self,x):\n","        h=self.shared(x)\n","        spk_mu, spk_log_std = self.speakerencoder(h)\n","        dec_in = torch.cat([spk_mu, spk_log_std], dim=1)\n","        spkresult=self.spkcls(dec_in)\n","    '''\n","    def loss_fn(self, outputs, x_gt):\n","        x_hat = outputs['x_hat']\n","        # emo_mu = outputs['emo_mu']\n","        # emo_log_std = outputs['emo_log_std']\n","        # spk_mu = outputs['spk_mu']\n","        # spk_log_std = outputs['spk_log_std']\n","        # emoresult=outputs['emocls']\n","        # spkresult=outputs['spkcls']\n","\n","        # emo_kl = kl_divergence(emo_mu, emo_log_std)\n","        # spk_kl = kl_divergence(spk_mu, spk_log_std)\n","\n","        # mi_loss=mutual_information(sample(emo_mu, emo_log_std),sample(spk_mu, spk_log_std))\n","        loss_2 = nn.MSELoss()\n","        nll = 0.5*loss_1(x_hat, x_gt) + 0.5*loss_2(x_hat, x_gt)\n","\n","        # emo_cls=loss_cls(emoresult, emo_gt)\n","        # spk_cls=loss_cls(spkresult,spk_gt)\n","\n","        return nll\n","\n"],"metadata":{"id":"Q7SYp7IrGGZH","executionInfo":{"status":"ok","timestamp":1738871574094,"user_tz":300,"elapsed":906,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["device=torch.device('cpu')"],"metadata":{"id":"uCk-GE1FP0aJ","executionInfo":{"status":"ok","timestamp":1738874786602,"user_tz":300,"elapsed":344,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# Create train_step()\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               #loss_fn: torch.nn.Module,\n","               optimizer:torch.optim.Optimizer,\n","               device=device):\n","  # Put the model in train mode\n","  model.train()\n","\n","  # Setup train loss and train accuracy values\n","  train_loss = 0\n","\n","  # Loop through data loader data batches\n","  for batch, tp in enumerate(dataloader):\n","    # Send data to the target device\n","    X=tp[:,:-2]\n","\n","    X_gt=X\n","\n","    #y=y.type(torch.LongTensor)\n","    X, X_gt = X.to(device), X_gt.to(device)\n","\n","    # 1. Forward pass\n","    y_pred = model(X) # output model logits\n","    #print(y_pred['spkcls'].shape)\n","    #y_pred = torch.softmax(y_pred,dim=1)\n","    #print(y_pred,y)\n","    # 2. Calculate the loss\n","    #y_pred=y_pred.type(torch.float32)\n","    nll = model.loss_fn(y_pred, X_gt)\n","    loss=nll\n","    train_loss += loss.item()\n","\n","    # 3. Optimizer zero grad\n","    optimizer.zero_grad()\n","\n","    # 4. Loss backward\n","    loss.backward()\n","\n","    # 5. Optimizer step\n","    optimizer.step()\n","\n","    # Calculate accuracy metric\n","    # y_pred_spkclass = torch.argmax(torch.softmax(y_pred['spkcls'], dim=1), dim=1)\n","    # spk_train_acc += (y_pred_spkclass==spk_gt).sum().item()/len(y_pred['spkcls'])\n","\n","    # y_pred_emoclass = torch.argmax(torch.softmax(y_pred['emocls'], dim=1), dim=1)\n","    # emo_train_acc += (y_pred_emoclass==emo_gt).sum().item()/len(y_pred['emocls'])\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  train_loss = train_loss / len(dataloader)\n","  # spk_train_acc = spk_train_acc / len(dataloader)\n","  # emo_train_acc = emo_train_acc / len(dataloader)\n","  return train_loss"],"metadata":{"id":"iWA285MIOkzb","executionInfo":{"status":"ok","timestamp":1738871582159,"user_tz":300,"elapsed":1,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Create a test step\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              #loss_fn: torch.nn.Module,\n","              device=device):\n","  # Put model in eval mode\n","  model.eval()\n","\n","  # Setup test loss and test accuracy values\n","  test_loss  = 0\n","\n","  # Turn on inference mode\n","  with torch.inference_mode():\n","    # Loop through DataLoader batches\n","    for batch, tp in enumerate(dataloader):\n","      # Send data to the target device\n","      X=tp[:,:-2]\n","\n","      X_gt=X\n","\n","      #y=y.type(torch.LongTensor)\n","      X, X_gt = X.to(device), X_gt.to(device)\n","\n","      # 1. Forward pass\n","      y_pred = model(X) # output model logits\n","\n","      #y_pred = torch.softmax(y_pred,dim=1)\n","      #print(y_pred,y)\n","      # 2. Calculate the loss\n","      #y_pred=y_pred.type(torch.float32)\n","      nll = model.loss_fn(y_pred, X_gt)\n","      loss=nll\n","      test_loss += loss.item()\n","\n","      # Calculate accuracy metric\n","      # y_pred_spkclass = torch.argmax(torch.softmax(y_pred['spkcls'], dim=1), dim=1)\n","      # spk_test_acc += (y_pred_spkclass==spk_gt).sum().item()/len(y_pred['spkcls'])\n","\n","      # y_pred_emoclass = torch.argmax(torch.softmax(y_pred['emocls'], dim=1), dim=1)\n","      # emo_test_acc += (y_pred_emoclass==emo_gt).sum().item()/len(y_pred['emocls'])\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","\n","  return test_loss\n","\n"],"metadata":{"id":"PhblWe3oOk-z","executionInfo":{"status":"ok","timestamp":1738871585464,"user_tz":300,"elapsed":258,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["\n","from tqdm.auto import tqdm\n","\n","# 1. Create a train function that takes in various model parameters + optimizer + dataloaders + loss function\n","def train(model: torch.nn.Module,\n","          train_dataloader,\n","          test_dataloader,\n","          optimizer,\n","          #loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n","          epochs: int = 5,\n","          device=device):\n","\n","  # 2. Create empty results dictionary\n","  results = {\"train_loss\": [],\n","             \"spk_train_acc\": [],\n","             \"emo_train_acc\": [],\n","             \"test_loss\": [],\n","             \"spk_test_acc\": [],\n","             \"emo_test_acc\": [],}\n","\n","  # 3. Loop through training and testing steps for a number of epochs\n","  for epoch in tqdm(range(epochs)):\n","    train_loss = train_step(model=model,\n","                                                          dataloader=train_dataloader,\n","                                                          #loss_fn=loss_fn,\n","                                                          optimizer=optimizer,\n","                                                          device=device)\n","    test_loss= test_step(model=model,\n","                                                      dataloader=test_dataloader,\n","                                                      #loss_fn=loss_fn,\n","                                                      device=device)\n","\n","    # 4. Print out what's happening\n","    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Test loss: {test_loss:.4f}\")\n","\n","    # 5. Update results dictionary\n","    results[\"train_loss\"].append(train_loss)\n","\n","  # 6. Return the filled results at the end of the epochs\n","  return results"],"metadata":{"id":"WqGtaV7AQL2x","executionInfo":{"status":"ok","timestamp":1738871588244,"user_tz":300,"elapsed":483,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["\n","# Set random seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Set number of epochs\n","NUM_EPOCHS = 400\n","\n","# Recreate an instance of TinyVGG\n","'''\n","model_0 = TinyVGG(input_shape=3, # number of color channels of our target images\n","                  hidden_units=10,\n","                  output_shape=len(train_data.classes)).to(device)\n","'''\n","model_main=Network_VAE(input_dim=192,\n","                        hidden_dim=64)\n","model_main.to(device)\n","# Setup loss function and optimizer\n","\n","optimizer = torch.optim.Adam(params=model_main.parameters(),\n","                             lr=0.0001)\n","\n","# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Train model_0\n","model_0_results = train(model=model_main,\n","                        train_dataloader=train_dataloader_simple,\n","                        test_dataloader=test_dataloader_simple,\n","                        optimizer=optimizer,\n","                        #loss_fn=loss_fn(),\n","                        epochs=NUM_EPOCHS)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["44c990cc35204d1495e18de22d9f5fed","74403c8d77814b8599734836929d9ec7","06577dd2ec114452822c584c7040c3fd","7bcf00c239ed461a913b6c083a7ab713","efac9ca6c88a4922a1994967957d14d7","f09be6ce50cf417fada2005594757fda","976222d5965c468b9d0bd6993c1b8315","44f3071c37794e36875edbc1f9df8bee","760f5a9124584f74abbf0ed758749c24","ce06a0ce70e24183a1cd88dcbbe98c8a","987085b9d90e43af83669a271272c831"]},"id":"8C6RJKGNQeFy","executionInfo":{"status":"ok","timestamp":1738871761786,"user_tz":300,"elapsed":168560,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"0edb6ee2-a46b-4edc-8a01-9e37469a6118"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/400 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c990cc35204d1495e18de22d9f5fed"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Train loss: 539.0059 | Test loss: 529.2979\n","Epoch: 1 | Train loss: 514.0065 | Test loss: 500.7938\n","Epoch: 2 | Train loss: 494.7468 | Test loss: 492.8026\n","Epoch: 3 | Train loss: 488.9831 | Test loss: 487.7838\n","Epoch: 4 | Train loss: 482.1388 | Test loss: 479.6309\n","Epoch: 5 | Train loss: 471.7778 | Test loss: 468.0294\n","Epoch: 6 | Train loss: 459.4836 | Test loss: 455.7639\n","Epoch: 7 | Train loss: 446.9577 | Test loss: 443.7916\n","Epoch: 8 | Train loss: 434.6117 | Test loss: 431.6837\n","Epoch: 9 | Train loss: 422.9335 | Test loss: 420.5024\n","Epoch: 10 | Train loss: 412.0003 | Test loss: 410.1878\n","Epoch: 11 | Train loss: 402.1258 | Test loss: 401.1795\n","Epoch: 12 | Train loss: 393.5039 | Test loss: 392.7429\n","Epoch: 13 | Train loss: 385.8195 | Test loss: 385.5854\n","Epoch: 14 | Train loss: 378.8753 | Test loss: 379.0601\n","Epoch: 15 | Train loss: 372.6448 | Test loss: 373.3309\n","Epoch: 16 | Train loss: 367.2365 | Test loss: 368.4598\n","Epoch: 17 | Train loss: 362.2685 | Test loss: 363.7787\n","Epoch: 18 | Train loss: 358.1895 | Test loss: 359.8711\n","Epoch: 19 | Train loss: 354.5766 | Test loss: 356.2259\n","Epoch: 20 | Train loss: 351.2843 | Test loss: 353.0267\n","Epoch: 21 | Train loss: 348.0973 | Test loss: 350.0006\n","Epoch: 22 | Train loss: 344.9392 | Test loss: 347.1908\n","Epoch: 23 | Train loss: 342.0549 | Test loss: 344.1885\n","Epoch: 24 | Train loss: 339.3380 | Test loss: 341.6889\n","Epoch: 25 | Train loss: 336.6548 | Test loss: 339.1819\n","Epoch: 26 | Train loss: 334.2297 | Test loss: 336.6179\n","Epoch: 27 | Train loss: 331.6963 | Test loss: 334.1422\n","Epoch: 28 | Train loss: 329.4753 | Test loss: 331.9270\n","Epoch: 29 | Train loss: 327.2859 | Test loss: 329.8016\n","Epoch: 30 | Train loss: 325.0570 | Test loss: 327.9492\n","Epoch: 31 | Train loss: 323.0468 | Test loss: 325.9535\n","Epoch: 32 | Train loss: 321.1286 | Test loss: 324.2446\n","Epoch: 33 | Train loss: 319.1891 | Test loss: 322.3920\n","Epoch: 34 | Train loss: 317.5485 | Test loss: 320.7656\n","Epoch: 35 | Train loss: 315.8576 | Test loss: 319.1096\n","Epoch: 36 | Train loss: 314.1520 | Test loss: 317.9545\n","Epoch: 37 | Train loss: 312.7173 | Test loss: 315.8794\n","Epoch: 38 | Train loss: 310.9054 | Test loss: 314.4256\n","Epoch: 39 | Train loss: 309.3701 | Test loss: 312.8716\n","Epoch: 40 | Train loss: 307.8093 | Test loss: 311.2820\n","Epoch: 41 | Train loss: 306.1760 | Test loss: 309.9636\n","Epoch: 42 | Train loss: 304.6678 | Test loss: 308.3029\n","Epoch: 43 | Train loss: 303.0923 | Test loss: 306.9905\n","Epoch: 44 | Train loss: 301.5619 | Test loss: 305.5026\n","Epoch: 45 | Train loss: 300.1067 | Test loss: 304.0899\n","Epoch: 46 | Train loss: 298.6121 | Test loss: 302.5182\n","Epoch: 47 | Train loss: 297.2050 | Test loss: 301.1522\n","Epoch: 48 | Train loss: 295.5820 | Test loss: 299.5757\n","Epoch: 49 | Train loss: 294.1941 | Test loss: 298.1453\n","Epoch: 50 | Train loss: 292.6234 | Test loss: 296.8710\n","Epoch: 51 | Train loss: 291.1272 | Test loss: 295.2591\n","Epoch: 52 | Train loss: 289.6193 | Test loss: 294.0998\n","Epoch: 53 | Train loss: 288.2428 | Test loss: 292.5459\n","Epoch: 54 | Train loss: 286.7672 | Test loss: 291.4348\n","Epoch: 55 | Train loss: 285.5223 | Test loss: 290.1393\n","Epoch: 56 | Train loss: 284.3044 | Test loss: 288.9566\n","Epoch: 57 | Train loss: 283.0690 | Test loss: 287.7566\n","Epoch: 58 | Train loss: 281.8156 | Test loss: 286.8469\n","Epoch: 59 | Train loss: 280.5369 | Test loss: 285.5552\n","Epoch: 60 | Train loss: 279.4701 | Test loss: 284.6475\n","Epoch: 61 | Train loss: 278.1138 | Test loss: 283.5066\n","Epoch: 62 | Train loss: 277.1156 | Test loss: 282.2568\n","Epoch: 63 | Train loss: 275.9509 | Test loss: 281.1673\n","Epoch: 64 | Train loss: 274.7766 | Test loss: 280.2869\n","Epoch: 65 | Train loss: 273.7506 | Test loss: 279.3648\n","Epoch: 66 | Train loss: 272.5379 | Test loss: 278.2196\n","Epoch: 67 | Train loss: 271.6104 | Test loss: 276.9978\n","Epoch: 68 | Train loss: 270.3261 | Test loss: 276.2469\n","Epoch: 69 | Train loss: 269.2594 | Test loss: 275.2629\n","Epoch: 70 | Train loss: 268.5190 | Test loss: 274.1232\n","Epoch: 71 | Train loss: 267.4109 | Test loss: 273.1388\n","Epoch: 72 | Train loss: 266.3147 | Test loss: 272.3506\n","Epoch: 73 | Train loss: 265.2585 | Test loss: 271.4653\n","Epoch: 74 | Train loss: 264.1930 | Test loss: 270.3987\n","Epoch: 75 | Train loss: 263.1899 | Test loss: 269.5293\n","Epoch: 76 | Train loss: 262.1942 | Test loss: 268.4092\n","Epoch: 77 | Train loss: 261.2486 | Test loss: 267.6993\n","Epoch: 78 | Train loss: 260.2289 | Test loss: 266.8570\n","Epoch: 79 | Train loss: 259.2075 | Test loss: 266.0995\n","Epoch: 80 | Train loss: 258.3628 | Test loss: 265.3381\n","Epoch: 81 | Train loss: 257.4151 | Test loss: 264.4998\n","Epoch: 82 | Train loss: 256.4653 | Test loss: 263.6203\n","Epoch: 83 | Train loss: 255.6532 | Test loss: 262.6174\n","Epoch: 84 | Train loss: 254.7053 | Test loss: 261.8581\n","Epoch: 85 | Train loss: 253.9618 | Test loss: 260.8344\n","Epoch: 86 | Train loss: 252.9884 | Test loss: 260.2058\n","Epoch: 87 | Train loss: 252.0684 | Test loss: 259.4268\n","Epoch: 88 | Train loss: 251.2664 | Test loss: 258.6427\n","Epoch: 89 | Train loss: 250.3635 | Test loss: 257.9223\n","Epoch: 90 | Train loss: 249.6448 | Test loss: 257.2597\n","Epoch: 91 | Train loss: 248.7640 | Test loss: 256.1834\n","Epoch: 92 | Train loss: 247.9561 | Test loss: 255.4859\n","Epoch: 93 | Train loss: 247.1427 | Test loss: 254.7624\n","Epoch: 94 | Train loss: 246.1989 | Test loss: 254.2236\n","Epoch: 95 | Train loss: 245.4542 | Test loss: 253.4955\n","Epoch: 96 | Train loss: 244.7613 | Test loss: 252.6231\n","Epoch: 97 | Train loss: 243.9725 | Test loss: 251.6832\n","Epoch: 98 | Train loss: 243.2401 | Test loss: 251.1779\n","Epoch: 99 | Train loss: 242.4213 | Test loss: 250.4599\n","Epoch: 100 | Train loss: 241.6059 | Test loss: 249.7262\n","Epoch: 101 | Train loss: 240.8796 | Test loss: 248.8661\n","Epoch: 102 | Train loss: 240.1046 | Test loss: 248.4140\n","Epoch: 103 | Train loss: 239.3783 | Test loss: 247.6057\n","Epoch: 104 | Train loss: 238.6338 | Test loss: 246.9322\n","Epoch: 105 | Train loss: 237.8659 | Test loss: 246.1244\n","Epoch: 106 | Train loss: 237.0712 | Test loss: 245.4451\n","Epoch: 107 | Train loss: 236.3967 | Test loss: 244.7552\n","Epoch: 108 | Train loss: 235.6077 | Test loss: 244.0838\n","Epoch: 109 | Train loss: 234.9560 | Test loss: 243.4242\n","Epoch: 110 | Train loss: 234.1376 | Test loss: 242.8022\n","Epoch: 111 | Train loss: 233.4323 | Test loss: 242.1230\n","Epoch: 112 | Train loss: 232.8089 | Test loss: 241.4690\n","Epoch: 113 | Train loss: 232.0370 | Test loss: 240.8888\n","Epoch: 114 | Train loss: 231.3281 | Test loss: 240.0629\n","Epoch: 115 | Train loss: 230.6801 | Test loss: 239.5113\n","Epoch: 116 | Train loss: 230.0227 | Test loss: 238.9149\n","Epoch: 117 | Train loss: 229.3814 | Test loss: 238.1634\n","Epoch: 118 | Train loss: 228.7593 | Test loss: 237.6331\n","Epoch: 119 | Train loss: 228.1741 | Test loss: 237.3458\n","Epoch: 120 | Train loss: 227.5733 | Test loss: 236.7636\n","Epoch: 121 | Train loss: 227.0408 | Test loss: 236.1349\n","Epoch: 122 | Train loss: 226.4559 | Test loss: 235.6389\n","Epoch: 123 | Train loss: 225.8921 | Test loss: 235.2238\n","Epoch: 124 | Train loss: 225.3575 | Test loss: 234.7443\n","Epoch: 125 | Train loss: 224.8814 | Test loss: 234.0459\n","Epoch: 126 | Train loss: 224.2000 | Test loss: 233.6392\n","Epoch: 127 | Train loss: 223.6305 | Test loss: 233.2342\n","Epoch: 128 | Train loss: 223.2588 | Test loss: 232.6437\n","Epoch: 129 | Train loss: 222.6164 | Test loss: 232.1084\n","Epoch: 130 | Train loss: 222.0940 | Test loss: 231.3755\n","Epoch: 131 | Train loss: 221.5267 | Test loss: 231.1001\n","Epoch: 132 | Train loss: 221.0109 | Test loss: 230.4514\n","Epoch: 133 | Train loss: 220.3853 | Test loss: 230.1102\n","Epoch: 134 | Train loss: 219.8453 | Test loss: 229.4680\n","Epoch: 135 | Train loss: 219.3936 | Test loss: 229.0968\n","Epoch: 136 | Train loss: 218.8868 | Test loss: 228.4606\n","Epoch: 137 | Train loss: 218.3764 | Test loss: 227.8944\n","Epoch: 138 | Train loss: 217.6692 | Test loss: 227.4555\n","Epoch: 139 | Train loss: 217.1888 | Test loss: 226.8410\n","Epoch: 140 | Train loss: 216.7186 | Test loss: 226.3570\n","Epoch: 141 | Train loss: 216.2231 | Test loss: 225.9263\n","Epoch: 142 | Train loss: 215.7102 | Test loss: 225.4009\n","Epoch: 143 | Train loss: 215.1986 | Test loss: 224.9737\n","Epoch: 144 | Train loss: 214.6221 | Test loss: 224.5819\n","Epoch: 145 | Train loss: 214.1579 | Test loss: 223.9211\n","Epoch: 146 | Train loss: 213.6224 | Test loss: 223.7192\n","Epoch: 147 | Train loss: 213.3767 | Test loss: 223.4123\n","Epoch: 148 | Train loss: 212.9486 | Test loss: 222.8900\n","Epoch: 149 | Train loss: 212.5171 | Test loss: 222.5496\n","Epoch: 150 | Train loss: 212.1921 | Test loss: 222.3015\n","Epoch: 151 | Train loss: 211.7532 | Test loss: 222.0168\n","Epoch: 152 | Train loss: 211.3931 | Test loss: 221.6675\n","Epoch: 153 | Train loss: 211.0572 | Test loss: 221.1551\n","Epoch: 154 | Train loss: 210.7521 | Test loss: 221.0151\n","Epoch: 155 | Train loss: 210.3990 | Test loss: 220.6168\n","Epoch: 156 | Train loss: 210.1105 | Test loss: 220.2439\n","Epoch: 157 | Train loss: 209.6818 | Test loss: 219.9801\n","Epoch: 158 | Train loss: 209.3786 | Test loss: 219.6353\n","Epoch: 159 | Train loss: 209.0117 | Test loss: 219.3119\n","Epoch: 160 | Train loss: 208.6574 | Test loss: 219.0118\n","Epoch: 161 | Train loss: 208.3368 | Test loss: 218.5484\n","Epoch: 162 | Train loss: 208.0585 | Test loss: 218.5486\n","Epoch: 163 | Train loss: 207.6914 | Test loss: 218.2041\n","Epoch: 164 | Train loss: 207.4064 | Test loss: 218.1219\n","Epoch: 165 | Train loss: 207.2102 | Test loss: 217.8656\n","Epoch: 166 | Train loss: 206.9571 | Test loss: 217.5275\n","Epoch: 167 | Train loss: 206.7399 | Test loss: 217.2551\n","Epoch: 168 | Train loss: 206.4332 | Test loss: 217.1592\n","Epoch: 169 | Train loss: 206.2036 | Test loss: 216.9598\n","Epoch: 170 | Train loss: 206.0368 | Test loss: 216.6054\n","Epoch: 171 | Train loss: 205.7065 | Test loss: 216.4954\n","Epoch: 172 | Train loss: 205.5600 | Test loss: 216.2105\n","Epoch: 173 | Train loss: 205.3078 | Test loss: 216.0422\n","Epoch: 174 | Train loss: 205.0784 | Test loss: 215.9905\n","Epoch: 175 | Train loss: 204.8983 | Test loss: 215.7760\n","Epoch: 176 | Train loss: 204.7107 | Test loss: 215.4971\n","Epoch: 177 | Train loss: 204.5705 | Test loss: 215.4807\n","Epoch: 178 | Train loss: 204.3074 | Test loss: 215.1765\n","Epoch: 179 | Train loss: 204.0437 | Test loss: 215.1266\n","Epoch: 180 | Train loss: 203.8835 | Test loss: 214.8582\n","Epoch: 181 | Train loss: 203.6604 | Test loss: 214.7440\n","Epoch: 182 | Train loss: 203.5292 | Test loss: 214.5483\n","Epoch: 183 | Train loss: 203.2050 | Test loss: 214.5967\n","Epoch: 184 | Train loss: 203.0663 | Test loss: 214.0332\n","Epoch: 185 | Train loss: 202.8874 | Test loss: 213.9113\n","Epoch: 186 | Train loss: 202.5916 | Test loss: 213.5786\n","Epoch: 187 | Train loss: 202.2034 | Test loss: 213.3397\n","Epoch: 188 | Train loss: 202.1434 | Test loss: 213.1418\n","Epoch: 189 | Train loss: 201.7108 | Test loss: 212.8859\n","Epoch: 190 | Train loss: 201.5104 | Test loss: 212.5420\n","Epoch: 191 | Train loss: 201.1679 | Test loss: 212.3232\n","Epoch: 192 | Train loss: 200.9025 | Test loss: 211.9898\n","Epoch: 193 | Train loss: 200.5747 | Test loss: 211.8256\n","Epoch: 194 | Train loss: 200.3354 | Test loss: 211.3212\n","Epoch: 195 | Train loss: 199.9158 | Test loss: 211.3151\n","Epoch: 196 | Train loss: 199.7749 | Test loss: 210.8715\n","Epoch: 197 | Train loss: 199.5216 | Test loss: 210.5706\n","Epoch: 198 | Train loss: 199.2329 | Test loss: 210.3504\n","Epoch: 199 | Train loss: 199.0336 | Test loss: 210.3069\n","Epoch: 200 | Train loss: 198.7280 | Test loss: 209.8373\n","Epoch: 201 | Train loss: 198.5180 | Test loss: 209.7302\n","Epoch: 202 | Train loss: 198.4126 | Test loss: 209.5347\n","Epoch: 203 | Train loss: 198.1919 | Test loss: 209.4349\n","Epoch: 204 | Train loss: 197.9478 | Test loss: 209.3986\n","Epoch: 205 | Train loss: 197.6394 | Test loss: 208.9501\n","Epoch: 206 | Train loss: 197.5693 | Test loss: 208.9480\n","Epoch: 207 | Train loss: 197.3414 | Test loss: 208.6703\n","Epoch: 208 | Train loss: 197.0463 | Test loss: 208.5598\n","Epoch: 209 | Train loss: 196.8296 | Test loss: 208.1767\n","Epoch: 210 | Train loss: 196.6019 | Test loss: 207.9768\n","Epoch: 211 | Train loss: 196.3334 | Test loss: 207.5903\n","Epoch: 212 | Train loss: 196.0866 | Test loss: 207.3000\n","Epoch: 213 | Train loss: 195.7070 | Test loss: 207.1577\n","Epoch: 214 | Train loss: 195.4937 | Test loss: 206.8818\n","Epoch: 215 | Train loss: 195.2718 | Test loss: 206.6965\n","Epoch: 216 | Train loss: 194.9780 | Test loss: 206.3547\n","Epoch: 217 | Train loss: 194.6531 | Test loss: 206.1200\n","Epoch: 218 | Train loss: 194.3673 | Test loss: 205.8440\n","Epoch: 219 | Train loss: 194.1040 | Test loss: 205.6381\n","Epoch: 220 | Train loss: 193.9253 | Test loss: 205.3904\n","Epoch: 221 | Train loss: 193.6939 | Test loss: 205.0585\n","Epoch: 222 | Train loss: 193.2951 | Test loss: 204.9589\n","Epoch: 223 | Train loss: 193.0789 | Test loss: 204.6823\n","Epoch: 224 | Train loss: 192.9248 | Test loss: 204.5013\n","Epoch: 225 | Train loss: 192.5916 | Test loss: 204.2245\n","Epoch: 226 | Train loss: 192.4529 | Test loss: 204.1403\n","Epoch: 227 | Train loss: 192.0689 | Test loss: 203.8535\n","Epoch: 228 | Train loss: 191.8427 | Test loss: 203.7616\n","Epoch: 229 | Train loss: 191.7157 | Test loss: 203.5024\n","Epoch: 230 | Train loss: 191.3425 | Test loss: 203.3208\n","Epoch: 231 | Train loss: 191.1876 | Test loss: 202.9669\n","Epoch: 232 | Train loss: 190.8939 | Test loss: 202.7388\n","Epoch: 233 | Train loss: 190.6639 | Test loss: 202.4492\n","Epoch: 234 | Train loss: 190.4034 | Test loss: 202.2517\n","Epoch: 235 | Train loss: 190.1808 | Test loss: 201.8566\n","Epoch: 236 | Train loss: 189.8895 | Test loss: 201.5968\n","Epoch: 237 | Train loss: 189.6471 | Test loss: 201.4832\n","Epoch: 238 | Train loss: 189.3393 | Test loss: 201.1807\n","Epoch: 239 | Train loss: 189.0346 | Test loss: 201.0642\n","Epoch: 240 | Train loss: 188.7835 | Test loss: 200.5500\n","Epoch: 241 | Train loss: 188.5373 | Test loss: 200.4555\n","Epoch: 242 | Train loss: 188.3453 | Test loss: 200.2708\n","Epoch: 243 | Train loss: 188.1427 | Test loss: 199.8943\n","Epoch: 244 | Train loss: 187.8869 | Test loss: 199.9054\n","Epoch: 245 | Train loss: 187.6640 | Test loss: 199.7139\n","Epoch: 246 | Train loss: 187.5397 | Test loss: 199.4970\n","Epoch: 247 | Train loss: 187.2902 | Test loss: 199.4003\n","Epoch: 248 | Train loss: 187.1373 | Test loss: 199.0889\n","Epoch: 249 | Train loss: 186.9415 | Test loss: 198.8439\n","Epoch: 250 | Train loss: 186.7496 | Test loss: 198.7844\n","Epoch: 251 | Train loss: 186.4693 | Test loss: 198.5770\n","Epoch: 252 | Train loss: 186.2452 | Test loss: 198.4731\n","Epoch: 253 | Train loss: 186.0758 | Test loss: 198.2439\n","Epoch: 254 | Train loss: 185.7609 | Test loss: 197.8613\n","Epoch: 255 | Train loss: 185.6519 | Test loss: 197.7855\n","Epoch: 256 | Train loss: 185.4220 | Test loss: 197.6834\n","Epoch: 257 | Train loss: 185.2684 | Test loss: 197.5091\n","Epoch: 258 | Train loss: 185.1450 | Test loss: 197.2083\n","Epoch: 259 | Train loss: 184.8662 | Test loss: 197.2994\n","Epoch: 260 | Train loss: 184.7853 | Test loss: 196.9932\n","Epoch: 261 | Train loss: 184.6825 | Test loss: 196.9041\n","Epoch: 262 | Train loss: 184.4814 | Test loss: 196.8601\n","Epoch: 263 | Train loss: 184.2769 | Test loss: 196.7250\n","Epoch: 264 | Train loss: 184.1229 | Test loss: 196.6875\n","Epoch: 265 | Train loss: 183.9660 | Test loss: 196.3405\n","Epoch: 266 | Train loss: 183.8776 | Test loss: 196.1706\n","Epoch: 267 | Train loss: 183.7445 | Test loss: 196.1411\n","Epoch: 268 | Train loss: 183.4642 | Test loss: 196.0532\n","Epoch: 269 | Train loss: 183.4303 | Test loss: 195.6994\n","Epoch: 270 | Train loss: 183.2189 | Test loss: 195.6054\n","Epoch: 271 | Train loss: 182.9705 | Test loss: 195.3890\n","Epoch: 272 | Train loss: 182.9007 | Test loss: 195.2469\n","Epoch: 273 | Train loss: 182.6030 | Test loss: 194.9836\n","Epoch: 274 | Train loss: 182.4920 | Test loss: 195.0799\n","Epoch: 275 | Train loss: 182.3885 | Test loss: 194.6687\n","Epoch: 276 | Train loss: 182.2204 | Test loss: 194.6980\n","Epoch: 277 | Train loss: 181.9973 | Test loss: 194.7092\n","Epoch: 278 | Train loss: 181.9721 | Test loss: 194.2870\n","Epoch: 279 | Train loss: 181.7174 | Test loss: 194.2895\n","Epoch: 280 | Train loss: 181.6119 | Test loss: 193.9418\n","Epoch: 281 | Train loss: 181.4467 | Test loss: 193.9192\n","Epoch: 282 | Train loss: 181.3491 | Test loss: 193.7580\n","Epoch: 283 | Train loss: 181.2717 | Test loss: 193.5894\n","Epoch: 284 | Train loss: 181.0417 | Test loss: 193.7150\n","Epoch: 285 | Train loss: 180.9645 | Test loss: 193.5621\n","Epoch: 286 | Train loss: 180.9565 | Test loss: 193.4809\n","Epoch: 287 | Train loss: 180.8393 | Test loss: 193.3798\n","Epoch: 288 | Train loss: 180.7426 | Test loss: 193.1448\n","Epoch: 289 | Train loss: 180.5291 | Test loss: 192.9668\n","Epoch: 290 | Train loss: 180.4852 | Test loss: 193.2005\n","Epoch: 291 | Train loss: 180.3754 | Test loss: 192.9311\n","Epoch: 292 | Train loss: 180.3186 | Test loss: 192.9461\n","Epoch: 293 | Train loss: 180.1986 | Test loss: 192.8126\n","Epoch: 294 | Train loss: 180.0989 | Test loss: 192.8685\n","Epoch: 295 | Train loss: 180.0612 | Test loss: 192.6872\n","Epoch: 296 | Train loss: 179.9152 | Test loss: 192.5982\n","Epoch: 297 | Train loss: 179.9620 | Test loss: 192.5267\n","Epoch: 298 | Train loss: 179.8406 | Test loss: 192.5276\n","Epoch: 299 | Train loss: 179.7290 | Test loss: 192.4241\n","Epoch: 300 | Train loss: 179.7429 | Test loss: 192.1625\n","Epoch: 301 | Train loss: 179.5772 | Test loss: 192.2118\n","Epoch: 302 | Train loss: 179.4259 | Test loss: 192.1217\n","Epoch: 303 | Train loss: 179.3090 | Test loss: 191.9564\n","Epoch: 304 | Train loss: 179.3218 | Test loss: 191.8452\n","Epoch: 305 | Train loss: 179.2705 | Test loss: 191.8887\n","Epoch: 306 | Train loss: 179.1195 | Test loss: 191.7616\n","Epoch: 307 | Train loss: 179.0840 | Test loss: 191.7031\n","Epoch: 308 | Train loss: 179.0084 | Test loss: 191.5370\n","Epoch: 309 | Train loss: 178.9966 | Test loss: 191.5584\n","Epoch: 310 | Train loss: 178.8696 | Test loss: 191.5012\n","Epoch: 311 | Train loss: 178.7346 | Test loss: 191.3719\n","Epoch: 312 | Train loss: 178.6940 | Test loss: 191.4790\n","Epoch: 313 | Train loss: 178.5620 | Test loss: 191.1342\n","Epoch: 314 | Train loss: 178.4704 | Test loss: 191.2094\n","Epoch: 315 | Train loss: 178.4563 | Test loss: 190.9958\n","Epoch: 316 | Train loss: 178.3134 | Test loss: 191.1192\n","Epoch: 317 | Train loss: 178.1414 | Test loss: 190.8051\n","Epoch: 318 | Train loss: 178.0605 | Test loss: 190.7553\n","Epoch: 319 | Train loss: 178.0564 | Test loss: 190.6985\n","Epoch: 320 | Train loss: 177.9158 | Test loss: 190.5528\n","Epoch: 321 | Train loss: 177.7338 | Test loss: 190.2210\n","Epoch: 322 | Train loss: 177.7308 | Test loss: 190.2635\n","Epoch: 323 | Train loss: 177.5303 | Test loss: 189.9716\n","Epoch: 324 | Train loss: 177.3695 | Test loss: 189.9221\n","Epoch: 325 | Train loss: 177.3332 | Test loss: 189.9596\n","Epoch: 326 | Train loss: 177.1656 | Test loss: 189.8638\n","Epoch: 327 | Train loss: 177.0672 | Test loss: 189.6793\n","Epoch: 328 | Train loss: 176.9223 | Test loss: 189.6465\n","Epoch: 329 | Train loss: 176.8742 | Test loss: 189.5943\n","Epoch: 330 | Train loss: 176.6817 | Test loss: 189.3473\n","Epoch: 331 | Train loss: 176.6643 | Test loss: 189.2506\n","Epoch: 332 | Train loss: 176.5372 | Test loss: 189.0514\n","Epoch: 333 | Train loss: 176.3799 | Test loss: 189.0513\n","Epoch: 334 | Train loss: 176.4185 | Test loss: 189.1999\n","Epoch: 335 | Train loss: 176.2649 | Test loss: 188.9055\n","Epoch: 336 | Train loss: 176.2455 | Test loss: 188.9835\n","Epoch: 337 | Train loss: 176.2175 | Test loss: 188.8409\n","Epoch: 338 | Train loss: 176.0640 | Test loss: 188.7408\n","Epoch: 339 | Train loss: 176.0261 | Test loss: 188.7449\n","Epoch: 340 | Train loss: 175.9068 | Test loss: 188.6700\n","Epoch: 341 | Train loss: 175.9711 | Test loss: 188.4266\n","Epoch: 342 | Train loss: 175.8101 | Test loss: 188.7131\n","Epoch: 343 | Train loss: 175.7122 | Test loss: 188.5722\n","Epoch: 344 | Train loss: 175.7786 | Test loss: 188.6743\n","Epoch: 345 | Train loss: 175.6826 | Test loss: 188.3441\n","Epoch: 346 | Train loss: 175.6609 | Test loss: 188.7587\n","Epoch: 347 | Train loss: 175.4708 | Test loss: 188.2800\n","Epoch: 348 | Train loss: 175.4969 | Test loss: 188.4822\n","Epoch: 349 | Train loss: 175.3645 | Test loss: 188.2831\n","Epoch: 350 | Train loss: 175.1757 | Test loss: 188.0663\n","Epoch: 351 | Train loss: 175.2238 | Test loss: 188.0092\n","Epoch: 352 | Train loss: 175.2014 | Test loss: 187.8071\n","Epoch: 353 | Train loss: 175.0467 | Test loss: 187.8778\n","Epoch: 354 | Train loss: 174.9815 | Test loss: 187.9648\n","Epoch: 355 | Train loss: 174.9395 | Test loss: 187.6425\n","Epoch: 356 | Train loss: 174.8101 | Test loss: 187.6903\n","Epoch: 357 | Train loss: 174.6853 | Test loss: 187.3405\n","Epoch: 358 | Train loss: 174.4033 | Test loss: 187.2887\n","Epoch: 359 | Train loss: 174.3575 | Test loss: 187.0660\n","Epoch: 360 | Train loss: 174.2981 | Test loss: 186.9242\n","Epoch: 361 | Train loss: 174.0820 | Test loss: 186.9195\n","Epoch: 362 | Train loss: 174.0360 | Test loss: 186.9174\n","Epoch: 363 | Train loss: 173.9709 | Test loss: 186.8084\n","Epoch: 364 | Train loss: 173.7942 | Test loss: 186.7067\n","Epoch: 365 | Train loss: 173.6230 | Test loss: 186.4235\n","Epoch: 366 | Train loss: 173.5384 | Test loss: 186.4867\n","Epoch: 367 | Train loss: 173.5023 | Test loss: 186.3212\n","Epoch: 368 | Train loss: 173.4192 | Test loss: 186.2741\n","Epoch: 369 | Train loss: 173.2871 | Test loss: 185.9652\n","Epoch: 370 | Train loss: 173.3267 | Test loss: 186.2541\n","Epoch: 371 | Train loss: 173.2023 | Test loss: 186.0757\n","Epoch: 372 | Train loss: 173.0698 | Test loss: 186.1183\n","Epoch: 373 | Train loss: 173.1798 | Test loss: 186.1334\n","Epoch: 374 | Train loss: 172.9499 | Test loss: 185.9878\n","Epoch: 375 | Train loss: 173.0344 | Test loss: 185.7902\n","Epoch: 376 | Train loss: 172.8740 | Test loss: 185.7432\n","Epoch: 377 | Train loss: 172.7956 | Test loss: 185.9231\n","Epoch: 378 | Train loss: 172.6882 | Test loss: 185.6740\n","Epoch: 379 | Train loss: 172.7240 | Test loss: 185.7083\n","Epoch: 380 | Train loss: 172.5891 | Test loss: 185.6770\n","Epoch: 381 | Train loss: 172.5722 | Test loss: 185.4496\n","Epoch: 382 | Train loss: 172.4806 | Test loss: 185.5081\n","Epoch: 383 | Train loss: 172.4539 | Test loss: 185.2683\n","Epoch: 384 | Train loss: 172.3189 | Test loss: 185.3827\n","Epoch: 385 | Train loss: 172.1933 | Test loss: 185.0638\n","Epoch: 386 | Train loss: 172.0785 | Test loss: 185.1283\n","Epoch: 387 | Train loss: 172.1509 | Test loss: 185.1339\n","Epoch: 388 | Train loss: 171.9710 | Test loss: 184.6767\n","Epoch: 389 | Train loss: 171.8046 | Test loss: 184.9878\n","Epoch: 390 | Train loss: 171.6853 | Test loss: 184.7567\n","Epoch: 391 | Train loss: 171.6126 | Test loss: 184.5876\n","Epoch: 392 | Train loss: 171.4498 | Test loss: 184.4545\n","Epoch: 393 | Train loss: 171.3073 | Test loss: 184.2731\n","Epoch: 394 | Train loss: 171.2079 | Test loss: 184.1417\n","Epoch: 395 | Train loss: 171.0935 | Test loss: 184.0199\n","Epoch: 396 | Train loss: 171.0158 | Test loss: 183.9102\n","Epoch: 397 | Train loss: 170.9800 | Test loss: 184.1939\n","Epoch: 398 | Train loss: 170.8756 | Test loss: 183.7739\n","Epoch: 399 | Train loss: 170.8763 | Test loss: 183.5754\n","Total training time: 167.185 seconds\n"]}]},{"cell_type":"code","source":["ans=None\n","model_main.to(\"cpu\")\n","model_main.eval()\n","with torch.inference_mode():\n","  for batch, tp in enumerate(test_dataloader_simple):\n","    # print(tp.shape)\n","    X=tp[:,:-2]\n","    gt=tp[:,-1]\n","\n","\n","\n","    X.to(device)\n","    # print(X.shape)\n","    txxx=model_main.get_feature(X)\n","    gt = torch.unsqueeze(gt, dim=-1)\n","    txxx=torch.cat((txxx,gt), axis=1)\n","    # print(txxx.shape)\n","    if ans is None:\n","      ans=txxx\n","    else:\n","      ans=torch.cat((ans,txxx), axis=0)\n","\n","torch.save(ans, \"iemecapa_VAE.pt\")"],"metadata":{"id":"eeYb4j6roDn1","executionInfo":{"status":"ok","timestamp":1738871784982,"user_tz":300,"elapsed":306,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["import torch.distributions as D\n","def sample(mu, logs, std=1.):\n","    \"\"\"\n","    :param mu: [batch, channel]\n","    :param logs:[batch, channel]\n","    :param std: sampling std\n","    :return: sample: [batch, channel]\n","    \"\"\"\n","    assert std > 0.\n","    eps = torch.normal(torch.zeros_like(mu), torch.ones_like(logs) * std)\n","    sample = eps * torch.exp(logs) + mu\n","    return sample\n","\n","def kl_divergence(mu, log_std):\n","    \"\"\"\n","    :param mu: [B, C]\n","    :param log_std: [B, C]\n","    :return:\n","    \"\"\"\n","    post = D.Normal(mu, torch.exp(log_std))\n","    prior = D.Normal(\n","        torch.zeros_like(mu, requires_grad=False),\n","        torch.ones_like(log_std, requires_grad=False))\n","    kl = D.kl.kl_divergence(post, prior)\n","    kl = torch.mean(torch.sum(kl, dim=1))\n","    return kl\n","\n","def loss_cls(x,y):\n","    loss_3=nn.CrossEntropyLoss()\n","    # 1. Forward pass\n","    x=x.type(torch.float32)\n","    # 2. Calculate the loss\n","    loss = loss_3(x, y.long())\n","    return loss\n"],"metadata":{"id":"7bMLQEezce9K","executionInfo":{"status":"ok","timestamp":1738869128832,"user_tz":300,"elapsed":337,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class L1SquaredLoss(nn.Module):\n","    def __init__(self, reduction='mean'):\n","        super(L1SquaredLoss, self).__init__()\n","        self.reduction = reduction\n","\n","    def forward(self, input, target):\n","        # Compute the absolute differences\n","        abs_diff = torch.abs(input - target)\n","        # Square the differences\n","        squared_diff = abs_diff ** 2\n","\n","        if self.reduction == 'mean':\n","            # Compute the mean of squared differences\n","            return torch.mean(squared_diff)\n","        elif self.reduction == 'sum':\n","            # Compute the sum of squared differences\n","            return torch.sum(squared_diff)\n","        else:\n","            # Return the squared differences as is\n","            return squared_diff\n","\n","# Example usage\n","#input = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","#target = torch.tensor([1.5, 2.5, 3.5])\n","\n","loss_1 = L1SquaredLoss(reduction='mean')\n","#loss = loss_fn(input, target)\n","\n","#print(\"L1 Squared Loss:\", loss.item())"],"metadata":{"id":"fSaog5AociG2","executionInfo":{"status":"ok","timestamp":1738869131292,"user_tz":300,"elapsed":323,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.distributions import Normal\n","\n","mean = torch.tensor([0.0, 0.0])\n","std = torch.tensor([1.0,1.0])\n","# normal_dist = Normal(mean, std)  # 创建一个均值为0，标准差为1的正态分布\n","sample = mean.rsample()  # 从正态分布中生成一个样本\n","print(sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"49n-2RGYI1Kb","executionInfo":{"status":"error","timestamp":1738869135404,"user_tz":300,"elapsed":1013,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"9932024c-50c0-4999-fbbc-a3a3dafbdf4b"},"execution_count":9,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'Tensor' object has no attribute 'rsample'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-dc25e9ef0d38>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# normal_dist = Normal(mean, std)  # 创建一个均值为0，标准差为1的正态分布\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 从正态分布中生成一个样本\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'rsample'"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"wuDGFd1fmCnB"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","\n","tp1=0\n","tp2=0\n","for i in range(1,6):\n","\n","  # x1=torch.load('iemdvector_sess' + str(i) +'_neu1.pt',map_location=torch.device('cpu'))\n","  # x2=np.loadtxt('iemdvector_sess' + str(i) +'_neu1.npy')+2*(i-1)\n","  # x1=torch.load('iemres_sess' + str(i) +'_neu1.pt',map_location=torch.device('cpu'))\n","  # x2=np.loadtxt('iemres_sess' + str(i) +'_neu1.npy')+2*(i-1)\n","  x1=torch.load('iemepaca_sess' + str(i) +'_neu1.pt',map_location=torch.device('cpu'))\n","  x2=np.loadtxt('iemepaca_sess' + str(i) +'_neu1.npy')+2*(i-1)\n","  print(x2.shape)\n","  if i==1:\n","    tp1=x1.detach().numpy()\n","    tp2=x2\n","  else:\n","    tp1=np.concatenate((tp1,x1.detach().numpy()), axis=0)\n","    tp2=np.concatenate((tp2,x2), axis=0)\n","\n","print(tp2.shape)\n","tp3=np.zeros((len(tp2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"235IVjV9mC41","executionInfo":{"status":"ok","timestamp":1738871522940,"user_tz":300,"elapsed":326,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"5afa038c-5d5a-4496-d166-8131b8a946c0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["(384,)\n","(360,)\n","(319,)\n","(257,)\n","(381,)\n","(1701,)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-19-7e3948380d0c>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  x1=torch.load('iemepaca_sess' + str(i) +'_neu1.pt',map_location=torch.device('cpu'))\n"]}]},{"cell_type":"code","source":["print(tp2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9Mg4V-AmOOo","executionInfo":{"status":"ok","timestamp":1738122646790,"user_tz":300,"elapsed":302,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"507ec6ca-0aff-4630-e788-c1f3caa9240f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1. 1. ... 8. 8. 8.]\n"]}]},{"cell_type":"code","source":["lis=['ang2','hap','sad','sur']\n","num=1\n","for j in lis:\n","  for i in range(1,6):\n","\n","    # x1=torch.load('iemdvector_sess' + str(i) +'_' + j + '.pt',map_location=torch.device('cpu'))\n","    # x2=np.loadtxt('iemdvector_sess' + str(i) +'_' + j + '.npy')+2*(i-1)\n","    x1=torch.load('iemecapa_sess' + str(i) +'_' + j + '.pt',map_location=torch.device('cpu'))\n","    x2=np.loadtxt('iemecapa_sess' + str(i) +'_' + j + '.npy')+2*(i-1)\n","    x3=np.zeros(len(x1))+num\n","    tp1=np.concatenate((tp1,x1.detach().numpy()), axis=0)\n","    tp2=np.concatenate((tp2,x2), axis=0)\n","    tp3=np.concatenate((tp3,x3), axis=0)\n","  num+=1\n","print(tp2.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eOGDC0omP_A","executionInfo":{"status":"ok","timestamp":1738871546213,"user_tz":300,"elapsed":1135,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"4ef3829d-f96c-4233-9125-e8862fd7954d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["(3974,)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-083127cd0ceb>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  x1=torch.load('iemecapa_sess' + str(i) +'_' + j + '.pt',map_location=torch.device('cpu'))\n"]}]},{"cell_type":"code","source":["tp11=torch.tensor(tp1,dtype=torch.float32)\n","\n","tp22=torch.from_numpy(tp2)\n","tp22=torch.tensor(tp22,dtype=torch.float32)\n","tp33=torch.from_numpy(tp3)\n","tp33=torch.tensor(tp33,dtype=torch.float32)\n","\n","test_dataset=torch.cat((tp11,tp33.unsqueeze(-1),tp22.unsqueeze(-1)), axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deZoCxxJmXfb","executionInfo":{"status":"ok","timestamp":1738871550084,"user_tz":300,"elapsed":322,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"7746cb5a-db75-4597-f0a8-d61c40a4194c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-21-1c8b21aba9a6>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  tp22=torch.tensor(tp22,dtype=torch.float32)\n","<ipython-input-21-1c8b21aba9a6>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  tp33=torch.tensor(tp33,dtype=torch.float32)\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Subset\n","from sklearn.model_selection import train_test_split\n","\n","TEST_SIZE = 0.1\n","BATCH_SIZE = 32\n","SEED = 42\n","\n","# generate indices: instead of the actual data we pass in integers instead\n","train_indices, test_indices, _, _ = train_test_split(\n","    range(len(test_dataset)),\n","    test_dataset[:,-1],\n","    stratify=test_dataset[:,-1],\n","    test_size=TEST_SIZE,\n","    random_state=SEED\n",")\n","\n","# generate subset based on indices\n","train_split = Subset(test_dataset, train_indices)\n","test_split = Subset(test_dataset, test_indices)\n","\n","# create batches\n","train_dataloader_simple = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataloader_simple = DataLoader(test_split, batch_size=BATCH_SIZE)"],"metadata":{"id":"Rkl66gr3nLac","executionInfo":{"status":"ok","timestamp":1738871554635,"user_tz":300,"elapsed":978,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"pM9lrWgTtLB8"}},{"cell_type":"markdown","source":["Beta-VAE"],"metadata":{"id":"-qNTAslBtLJH"}},{"cell_type":"code","source":["import torch.nn as nn\n","class Beta_VAE(nn.Module):\n","    def __init__(self, input_dim=256, output_dim=256, num_domains=2, hidden_dim=384,emoclassnum=5,spkclassnum=10):\n","        super().__init__()\n","        self.output_dim=output_dim\n","        layers = []\n","        layers += [nn.Linear(input_dim, hidden_dim)]\n","        layers += [nn.ReLU()]\n","        for _ in range(2):\n","            layers += [nn.Linear(hidden_dim, hidden_dim)]\n","            layers += [nn.LayerNorm(hidden_dim)]\n","        layers +=[nn.Linear(hidden_dim, output_dim * 2)]\n","        layers +=[nn.LayerNorm(output_dim * 2)]\n","\n","\n","        self.shared = nn.Sequential(*layers)\n","\n","        # self.speakerencoder=nn.Sequential(*[nn.Linear(hidden_dim, hidden_dim),\n","        #                                     nn.LayerNorm(hidden_dim),\n","        #                                     nn.Linear(hidden_dim, output_dim * 2),\n","        #                                     nn.LayerNorm(hidden_dim)])\n","\n","        # self.emotionencoder=nn.Sequential(*[nn.Linear(hidden_dim, hidden_dim),\n","        #                                     nn.LayerNorm(hidden_dim),\n","        #                                     nn.Linear(hidden_dim, output_dim * 2),\n","        #                                     nn.LayerNorm(hidden_dim)])\n","\n","        self.decoder=nn.Sequential(*[nn.Linear(output_dim, hidden_dim),\n","                                      nn.ReLU(),\n","                                      nn.Linear(hidden_dim, input_dim)]\n","                                   )\n","\n","        # self.emocls = nn.Sequential(*[nn.Linear(output_dim * 2, emoclassnum)])\n","\n","        # self.spkcls = nn.Sequential(*[nn.Linear(output_dim * 2, spkclassnum)])\n","\n","\n","\n","        '''\n","        self.unshared = nn.ModuleList()\n","        for _ in range(num_domains):\n","            self.unshared += [nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n","                                            nn.ReLU(),\n","                                            nn.Linear(hidden_dim, hidden_dim),\n","                                            nn.ReLU(),\n","                                            nn.Linear(hidden_dim, style_dim))]\n","        '''\n","    def forward(self, x):\n","        h=self.shared(x)\n","\n","        # emo_mu_logstd=self.emotionencoder(h)\n","        # spk_mu_logstd=self.speakerencoder(h)\n","\n","        # emo_mu, emo_log_std = torch.split(emo_mu_logstd,self.output_dim, dim=1)\n","        # spk_mu, spk_log_std = torch.split(spk_mu_logstd,self.output_dim, dim=1)\n","\n","        mu, log_std = torch.split(h,self.output_dim, dim=1)\n","\n","        z = sample(mu, log_std)\n","\n","        x_hat = self.decoder(z)\n","\n","        return {'x_hat': x_hat, 'mu': mu, 'log_std': log_std,'z': z}\n","\n","\n","    def get_feature(self,x):\n","        h=self.shared(x)\n","\n","        # spk_mu_logstd=self.speakerencoder(h)\n","\n","        return h\n","    '''\n","    def get_emo(self,x):\n","        h=self.shared(x)\n","        emo_mu, emo_log_std = self.emotionencoder(h)\n","        dec_in = torch.cat([emo_mu, emo_log_std], dim=1)\n","        emoresult=self.emocls(dec_in)\n","\n","    def get_spk(self,x):\n","        h=self.shared(x)\n","        spk_mu, spk_log_std = self.speakerencoder(h)\n","        dec_in = torch.cat([spk_mu, spk_log_std], dim=1)\n","        spkresult=self.spkcls(dec_in)\n","    '''\n","    def loss_fn(self, outputs, x_gt):\n","        x_hat = outputs['x_hat']\n","        mu = outputs['mu']\n","        log_std = outputs['log_std']\n","        # spk_mu = outputs['spk_mu']\n","        # spk_log_std = outputs['spk_log_std']\n","        # emoresult=outputs['emocls']\n","        # spkresult=outputs['spkcls']\n","\n","        kl = kl_divergence(mu, log_std)\n","        # spk_kl = kl_divergence(spk_mu, spk_log_std)\n","\n","        # mi_loss=mutual_information(sample(emo_mu, emo_log_std),sample(spk_mu, spk_log_std))\n","        loss_2 = nn.MSELoss()\n","        nll = 0.5*loss_1(x_hat, x_gt) + 0.5*loss_2(x_hat, x_gt)\n","\n","        # emo_cls=loss_cls(emoresult, emo_gt)\n","        # spk_cls=loss_cls(spkresult,spk_gt)\n","\n","        return nll,kl\n","\n","# Create train_step()\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               #loss_fn: torch.nn.Module,\n","               optimizer:torch.optim.Optimizer,\n","               device=device):\n","  # Put the model in train mode\n","  model.train()\n","\n","  # Setup train loss and train accuracy values\n","  train_loss = 0\n","\n","  # Loop through data loader data batches\n","  for batch, tp in enumerate(dataloader):\n","    # Send data to the target device\n","    X=tp[:,:-2]\n","\n","    X_gt=X\n","\n","    #y=y.type(torch.LongTensor)\n","    X, X_gt = X.to(device), X_gt.to(device)\n","\n","    # 1. Forward pass\n","    y_pred = model(X) # output model logits\n","    #print(y_pred['spkcls'].shape)\n","    #y_pred = torch.softmax(y_pred,dim=1)\n","    #print(y_pred,y)\n","    # 2. Calculate the loss\n","    #y_pred=y_pred.type(torch.float32)\n","    nll,kl = model.loss_fn(y_pred, X_gt)\n","    loss=nll + beta * kl\n","    train_loss += loss.item()\n","\n","    # 3. Optimizer zero grad\n","    optimizer.zero_grad()\n","\n","    # 4. Loss backward\n","    loss.backward()\n","\n","    # 5. Optimizer step\n","    optimizer.step()\n","\n","    # Calculate accuracy metric\n","    # y_pred_spkclass = torch.argmax(torch.softmax(y_pred['spkcls'], dim=1), dim=1)\n","    # spk_train_acc += (y_pred_spkclass==spk_gt).sum().item()/len(y_pred['spkcls'])\n","\n","    # y_pred_emoclass = torch.argmax(torch.softmax(y_pred['emocls'], dim=1), dim=1)\n","    # emo_train_acc += (y_pred_emoclass==emo_gt).sum().item()/len(y_pred['emocls'])\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  train_loss = train_loss / len(dataloader)\n","  # spk_train_acc = spk_train_acc / len(dataloader)\n","  # emo_train_acc = emo_train_acc / len(dataloader)\n","  return train_loss\n","\n","# Create a test step\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              #loss_fn: torch.nn.Module,\n","              device=device):\n","  # Put model in eval mode\n","  model.eval()\n","\n","  # Setup test loss and test accuracy values\n","  test_loss  = 0\n","\n","  # Turn on inference mode\n","  with torch.inference_mode():\n","    # Loop through DataLoader batches\n","    for batch, tp in enumerate(dataloader):\n","      # Send data to the target device\n","      X=tp[:,:-2]\n","\n","      X_gt=X\n","\n","      #y=y.type(torch.LongTensor)\n","      X, X_gt = X.to(device), X_gt.to(device)\n","\n","      # 1. Forward pass\n","      y_pred = model(X) # output model logits\n","\n","      #y_pred = torch.softmax(y_pred,dim=1)\n","      #print(y_pred,y)\n","      # 2. Calculate the loss\n","      #y_pred=y_pred.type(torch.float32)\n","      nll,kl = model.loss_fn(y_pred, X_gt)\n","      loss=nll + beta * kl\n","      test_loss += loss.item()\n","\n","      # Calculate accuracy metric\n","      # y_pred_spkclass = torch.argmax(torch.softmax(y_pred['spkcls'], dim=1), dim=1)\n","      # spk_test_acc += (y_pred_spkclass==spk_gt).sum().item()/len(y_pred['spkcls'])\n","\n","      # y_pred_emoclass = torch.argmax(torch.softmax(y_pred['emocls'], dim=1), dim=1)\n","      # emo_test_acc += (y_pred_emoclass==emo_gt).sum().item()/len(y_pred['emocls'])\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","\n","  return test_loss\n","\n","\n","\n","from tqdm.auto import tqdm\n","\n","# 1. Create a train function that takes in various model parameters + optimizer + dataloaders + loss function\n","def train(model: torch.nn.Module,\n","          train_dataloader,\n","          test_dataloader,\n","          optimizer,\n","          #loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n","          epochs: int = 5,\n","          device=device):\n","\n","  # 2. Create empty results dictionary\n","  results = {\"train_loss\": [],\n","             \"spk_train_acc\": [],\n","             \"emo_train_acc\": [],\n","             \"test_loss\": [],\n","             \"spk_test_acc\": [],\n","             \"emo_test_acc\": [],}\n","\n","  # 3. Loop through training and testing steps for a number of epochs\n","  for epoch in tqdm(range(epochs)):\n","    train_loss = train_step(model=model,\n","                            dataloader=train_dataloader,\n","                            #loss_fn=loss_fn,\n","                            optimizer=optimizer,\n","                            device=device)\n","    test_loss= test_step(model=model,\n","                        dataloader=test_dataloader,\n","                        #loss_fn=loss_fn,\n","                        device=device)\n","\n","    # 4. Print out what's happening\n","    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Test loss: {test_loss:.4f}\")\n","\n","    # 5. Update results dictionary\n","    results[\"train_loss\"].append(train_loss)\n","\n","  # 6. Return the filled results at the end of the epochs\n","  return results\n"],"metadata":{"id":"YPZbIRTbtObg","executionInfo":{"status":"ok","timestamp":1738871814567,"user_tz":300,"elapsed":1226,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["\n","# Set random seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Set number of epochs\n","NUM_EPOCHS = 400\n","beta=4.0\n","\n","# Recreate an instance of TinyVGG\n","'''\n","model_0 = TinyVGG(input_shape=3, # number of color channels of our target images\n","                  hidden_units=10,\n","                  output_shape=len(train_data.classes)).to(device)\n","'''\n","model_main=Beta_VAE(input_dim=192,\n","                        hidden_dim=64)\n","model_main.to(device)\n","# Setup loss function and optimizer\n","\n","optimizer = torch.optim.Adam(params=model_main.parameters(),\n","                             lr=0.0001)\n","\n","# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Train model_0\n","model_0_results = train(model=model_main,\n","                        train_dataloader=train_dataloader_simple,\n","                        test_dataloader=test_dataloader_simple,\n","                        optimizer=optimizer,\n","                        #loss_fn=loss_fn(),\n","                        epochs=NUM_EPOCHS)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a00a882bcaf34443af9083df69b222bf","933edece347d499bb3b4ff26f6d38c86","9b406b2416fa40fe838597ed66dd624d","dd89de1385a74ab18b4bfc089a698168","903e326d2f474df8b329a00a24e17bb2","0e6a1f1bb14d46e2be0eb69c12ddc505","32a20dc32bba483aa1f8afb29a89acf9","2f9e0dcfb41e4a01b33196288c981335","71c674551a6a469a8f9a62f3907cf425","7e916ced43834fc083ba3d10e853151d","5cf88d6c79e844bba6da07c56bf1f42b"]},"id":"rGNJHyhq1uTQ","executionInfo":{"status":"ok","timestamp":1738872021431,"user_tz":300,"elapsed":202999,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"0a12a36f-51fb-4a42-cc6b-72d304b38522"},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/400 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00a882bcaf34443af9083df69b222bf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Train loss: 2346.3972 | Test loss: 1633.1527\n","Epoch: 1 | Train loss: 1482.7758 | Test loss: 1364.2488\n","Epoch: 2 | Train loss: 1269.7896 | Test loss: 1180.2846\n","Epoch: 3 | Train loss: 1106.8935 | Test loss: 1045.0126\n","Epoch: 4 | Train loss: 999.9118 | Test loss: 964.3182\n","Epoch: 5 | Train loss: 936.7881 | Test loss: 914.2553\n","Epoch: 6 | Train loss: 891.8576 | Test loss: 871.7898\n","Epoch: 7 | Train loss: 849.5869 | Test loss: 830.6195\n","Epoch: 8 | Train loss: 810.2875 | Test loss: 793.5365\n","Epoch: 9 | Train loss: 777.6366 | Test loss: 764.2899\n","Epoch: 10 | Train loss: 750.0001 | Test loss: 739.2205\n","Epoch: 11 | Train loss: 726.3932 | Test loss: 717.2972\n","Epoch: 12 | Train loss: 706.8942 | Test loss: 697.8740\n","Epoch: 13 | Train loss: 686.5570 | Test loss: 678.4509\n","Epoch: 14 | Train loss: 668.1737 | Test loss: 661.7344\n","Epoch: 15 | Train loss: 654.0659 | Test loss: 649.1614\n","Epoch: 16 | Train loss: 643.4988 | Test loss: 640.7759\n","Epoch: 17 | Train loss: 633.1732 | Test loss: 629.9085\n","Epoch: 18 | Train loss: 622.6836 | Test loss: 618.7378\n","Epoch: 19 | Train loss: 615.3343 | Test loss: 611.6480\n","Epoch: 20 | Train loss: 608.2655 | Test loss: 605.4419\n","Epoch: 21 | Train loss: 599.5263 | Test loss: 597.1263\n","Epoch: 22 | Train loss: 591.5020 | Test loss: 589.7867\n","Epoch: 23 | Train loss: 584.7743 | Test loss: 581.8659\n","Epoch: 24 | Train loss: 578.4259 | Test loss: 575.7515\n","Epoch: 25 | Train loss: 570.5235 | Test loss: 568.3249\n","Epoch: 26 | Train loss: 563.0893 | Test loss: 560.6675\n","Epoch: 27 | Train loss: 557.2463 | Test loss: 555.5760\n","Epoch: 28 | Train loss: 554.8487 | Test loss: 556.1864\n","Epoch: 29 | Train loss: 552.8172 | Test loss: 552.4962\n","Epoch: 30 | Train loss: 550.8819 | Test loss: 551.5006\n","Epoch: 31 | Train loss: 549.2380 | Test loss: 549.8614\n","Epoch: 32 | Train loss: 548.1995 | Test loss: 549.0034\n","Epoch: 33 | Train loss: 546.6694 | Test loss: 546.4228\n","Epoch: 34 | Train loss: 545.6237 | Test loss: 545.4964\n","Epoch: 35 | Train loss: 543.3762 | Test loss: 544.0915\n","Epoch: 36 | Train loss: 542.1881 | Test loss: 541.6840\n","Epoch: 37 | Train loss: 540.7549 | Test loss: 540.8397\n","Epoch: 38 | Train loss: 539.5189 | Test loss: 540.3801\n","Epoch: 39 | Train loss: 537.7994 | Test loss: 537.6195\n","Epoch: 40 | Train loss: 536.3920 | Test loss: 537.0882\n","Epoch: 41 | Train loss: 535.2522 | Test loss: 535.9024\n","Epoch: 42 | Train loss: 533.2161 | Test loss: 533.8205\n","Epoch: 43 | Train loss: 532.1768 | Test loss: 533.0844\n","Epoch: 44 | Train loss: 530.4923 | Test loss: 530.4501\n","Epoch: 45 | Train loss: 528.8763 | Test loss: 529.7386\n","Epoch: 46 | Train loss: 527.3961 | Test loss: 527.6506\n","Epoch: 47 | Train loss: 526.0368 | Test loss: 525.7440\n","Epoch: 48 | Train loss: 524.7647 | Test loss: 525.0492\n","Epoch: 49 | Train loss: 523.1698 | Test loss: 524.3170\n","Epoch: 50 | Train loss: 521.5700 | Test loss: 522.1842\n","Epoch: 51 | Train loss: 520.2905 | Test loss: 520.8805\n","Epoch: 52 | Train loss: 519.0585 | Test loss: 519.3926\n","Epoch: 53 | Train loss: 517.2317 | Test loss: 518.4952\n","Epoch: 54 | Train loss: 515.4180 | Test loss: 515.8381\n","Epoch: 55 | Train loss: 514.1338 | Test loss: 514.8707\n","Epoch: 56 | Train loss: 512.6079 | Test loss: 513.2935\n","Epoch: 57 | Train loss: 511.3162 | Test loss: 512.9406\n","Epoch: 58 | Train loss: 509.9888 | Test loss: 511.1953\n","Epoch: 59 | Train loss: 508.3746 | Test loss: 509.4473\n","Epoch: 60 | Train loss: 506.9236 | Test loss: 507.3509\n","Epoch: 61 | Train loss: 505.3287 | Test loss: 506.2805\n","Epoch: 62 | Train loss: 504.0282 | Test loss: 505.4988\n","Epoch: 63 | Train loss: 502.6465 | Test loss: 503.6054\n","Epoch: 64 | Train loss: 500.9560 | Test loss: 502.4244\n","Epoch: 65 | Train loss: 500.0498 | Test loss: 501.1601\n","Epoch: 66 | Train loss: 498.7285 | Test loss: 499.9517\n","Epoch: 67 | Train loss: 497.6840 | Test loss: 499.0619\n","Epoch: 68 | Train loss: 496.7318 | Test loss: 497.9998\n","Epoch: 69 | Train loss: 495.9275 | Test loss: 498.0845\n","Epoch: 70 | Train loss: 495.5885 | Test loss: 496.9498\n","Epoch: 71 | Train loss: 495.4533 | Test loss: 497.2682\n","Epoch: 72 | Train loss: 494.8497 | Test loss: 496.9017\n","Epoch: 73 | Train loss: 494.6730 | Test loss: 497.2070\n","Epoch: 74 | Train loss: 494.3633 | Test loss: 496.1434\n","Epoch: 75 | Train loss: 494.1118 | Test loss: 496.5608\n","Epoch: 76 | Train loss: 493.8310 | Test loss: 495.5041\n","Epoch: 77 | Train loss: 493.6472 | Test loss: 494.9635\n","Epoch: 78 | Train loss: 493.3035 | Test loss: 495.2436\n","Epoch: 79 | Train loss: 492.5554 | Test loss: 495.2050\n","Epoch: 80 | Train loss: 492.5172 | Test loss: 494.0140\n","Epoch: 81 | Train loss: 491.5659 | Test loss: 494.2753\n","Epoch: 82 | Train loss: 490.8206 | Test loss: 492.5679\n","Epoch: 83 | Train loss: 490.2411 | Test loss: 492.0908\n","Epoch: 84 | Train loss: 489.4603 | Test loss: 491.5956\n","Epoch: 85 | Train loss: 489.0327 | Test loss: 491.2246\n","Epoch: 86 | Train loss: 488.2340 | Test loss: 489.9964\n","Epoch: 87 | Train loss: 487.4628 | Test loss: 489.7750\n","Epoch: 88 | Train loss: 486.6458 | Test loss: 490.3186\n","Epoch: 89 | Train loss: 486.0728 | Test loss: 488.8839\n","Epoch: 90 | Train loss: 485.4872 | Test loss: 488.3351\n","Epoch: 91 | Train loss: 485.2159 | Test loss: 487.7616\n","Epoch: 92 | Train loss: 484.5916 | Test loss: 488.6599\n","Epoch: 93 | Train loss: 483.9988 | Test loss: 487.4407\n","Epoch: 94 | Train loss: 483.9091 | Test loss: 486.8887\n","Epoch: 95 | Train loss: 483.5905 | Test loss: 487.6696\n","Epoch: 96 | Train loss: 483.2578 | Test loss: 486.7553\n","Epoch: 97 | Train loss: 483.0454 | Test loss: 487.4312\n","Epoch: 98 | Train loss: 482.7715 | Test loss: 487.4468\n","Epoch: 99 | Train loss: 482.4940 | Test loss: 486.5895\n","Epoch: 100 | Train loss: 482.3720 | Test loss: 486.3485\n","Epoch: 101 | Train loss: 482.0366 | Test loss: 485.3753\n","Epoch: 102 | Train loss: 482.2405 | Test loss: 485.9248\n","Epoch: 103 | Train loss: 482.0950 | Test loss: 485.8393\n","Epoch: 104 | Train loss: 482.0341 | Test loss: 485.3673\n","Epoch: 105 | Train loss: 481.7728 | Test loss: 485.4769\n","Epoch: 106 | Train loss: 482.0137 | Test loss: 486.0153\n","Epoch: 107 | Train loss: 481.4780 | Test loss: 485.3000\n","Epoch: 108 | Train loss: 481.7492 | Test loss: 486.5337\n","Epoch: 109 | Train loss: 481.7480 | Test loss: 485.0390\n","Epoch: 110 | Train loss: 481.1165 | Test loss: 483.8241\n","Epoch: 111 | Train loss: 481.1860 | Test loss: 485.3185\n","Epoch: 112 | Train loss: 481.4998 | Test loss: 484.9979\n","Epoch: 113 | Train loss: 480.9603 | Test loss: 485.0286\n","Epoch: 114 | Train loss: 480.7355 | Test loss: 485.6425\n","Epoch: 115 | Train loss: 481.1223 | Test loss: 484.4569\n","Epoch: 116 | Train loss: 480.6834 | Test loss: 484.0829\n","Epoch: 117 | Train loss: 480.9516 | Test loss: 484.9188\n","Epoch: 118 | Train loss: 480.5270 | Test loss: 482.9621\n","Epoch: 119 | Train loss: 480.8576 | Test loss: 485.0773\n","Epoch: 120 | Train loss: 480.4499 | Test loss: 483.9478\n","Epoch: 121 | Train loss: 480.3233 | Test loss: 484.4465\n","Epoch: 122 | Train loss: 480.3683 | Test loss: 483.8173\n","Epoch: 123 | Train loss: 480.2335 | Test loss: 483.5203\n","Epoch: 124 | Train loss: 480.2902 | Test loss: 484.0827\n","Epoch: 125 | Train loss: 480.3423 | Test loss: 483.9502\n","Epoch: 126 | Train loss: 480.0453 | Test loss: 483.8225\n","Epoch: 127 | Train loss: 479.7669 | Test loss: 484.0908\n","Epoch: 128 | Train loss: 479.4626 | Test loss: 482.8959\n","Epoch: 129 | Train loss: 479.5308 | Test loss: 484.0529\n","Epoch: 130 | Train loss: 479.2167 | Test loss: 483.3437\n","Epoch: 131 | Train loss: 479.1319 | Test loss: 483.1216\n","Epoch: 132 | Train loss: 478.2262 | Test loss: 482.4995\n","Epoch: 133 | Train loss: 478.2500 | Test loss: 480.9150\n","Epoch: 134 | Train loss: 477.7784 | Test loss: 481.3543\n","Epoch: 135 | Train loss: 477.3171 | Test loss: 481.8479\n","Epoch: 136 | Train loss: 477.0023 | Test loss: 480.8354\n","Epoch: 137 | Train loss: 476.2233 | Test loss: 480.7244\n","Epoch: 138 | Train loss: 475.5643 | Test loss: 479.7836\n","Epoch: 139 | Train loss: 474.7957 | Test loss: 478.5766\n","Epoch: 140 | Train loss: 473.8982 | Test loss: 477.5813\n","Epoch: 141 | Train loss: 473.5692 | Test loss: 476.6520\n","Epoch: 142 | Train loss: 472.8429 | Test loss: 476.6669\n","Epoch: 143 | Train loss: 472.0411 | Test loss: 476.2877\n","Epoch: 144 | Train loss: 471.6876 | Test loss: 476.3057\n","Epoch: 145 | Train loss: 471.1035 | Test loss: 474.6146\n","Epoch: 146 | Train loss: 470.2932 | Test loss: 475.2283\n","Epoch: 147 | Train loss: 470.0636 | Test loss: 474.0085\n","Epoch: 148 | Train loss: 469.9616 | Test loss: 474.3474\n","Epoch: 149 | Train loss: 469.6222 | Test loss: 474.3283\n","Epoch: 150 | Train loss: 469.3479 | Test loss: 472.7564\n","Epoch: 151 | Train loss: 468.7193 | Test loss: 473.5269\n","Epoch: 152 | Train loss: 468.6330 | Test loss: 473.4383\n","Epoch: 153 | Train loss: 468.4530 | Test loss: 472.1878\n","Epoch: 154 | Train loss: 468.0941 | Test loss: 473.1029\n","Epoch: 155 | Train loss: 468.0515 | Test loss: 472.1327\n","Epoch: 156 | Train loss: 468.2801 | Test loss: 472.7001\n","Epoch: 157 | Train loss: 468.1338 | Test loss: 471.1910\n","Epoch: 158 | Train loss: 467.7611 | Test loss: 472.2901\n","Epoch: 159 | Train loss: 467.6677 | Test loss: 471.9202\n","Epoch: 160 | Train loss: 467.5772 | Test loss: 472.9854\n","Epoch: 161 | Train loss: 467.4704 | Test loss: 471.3339\n","Epoch: 162 | Train loss: 466.9238 | Test loss: 471.8097\n","Epoch: 163 | Train loss: 467.2604 | Test loss: 471.2772\n","Epoch: 164 | Train loss: 467.1202 | Test loss: 471.3656\n","Epoch: 165 | Train loss: 467.1149 | Test loss: 471.8444\n","Epoch: 166 | Train loss: 466.8426 | Test loss: 472.2374\n","Epoch: 167 | Train loss: 466.5375 | Test loss: 471.4108\n","Epoch: 168 | Train loss: 466.4634 | Test loss: 470.4070\n","Epoch: 169 | Train loss: 466.6618 | Test loss: 471.8963\n","Epoch: 170 | Train loss: 466.3489 | Test loss: 472.1215\n","Epoch: 171 | Train loss: 466.2106 | Test loss: 470.3886\n","Epoch: 172 | Train loss: 466.1418 | Test loss: 470.6571\n","Epoch: 173 | Train loss: 465.4710 | Test loss: 469.7039\n","Epoch: 174 | Train loss: 465.7582 | Test loss: 471.0548\n","Epoch: 175 | Train loss: 465.4082 | Test loss: 470.5088\n","Epoch: 176 | Train loss: 465.4323 | Test loss: 470.9988\n","Epoch: 177 | Train loss: 465.6615 | Test loss: 470.2198\n","Epoch: 178 | Train loss: 465.2732 | Test loss: 469.4365\n","Epoch: 179 | Train loss: 464.6296 | Test loss: 468.5796\n","Epoch: 180 | Train loss: 464.7953 | Test loss: 469.0003\n","Epoch: 181 | Train loss: 463.9661 | Test loss: 469.2479\n","Epoch: 182 | Train loss: 463.9609 | Test loss: 468.1486\n","Epoch: 183 | Train loss: 463.6357 | Test loss: 468.2560\n","Epoch: 184 | Train loss: 463.2451 | Test loss: 467.8046\n","Epoch: 185 | Train loss: 463.4176 | Test loss: 468.1475\n","Epoch: 186 | Train loss: 462.6353 | Test loss: 467.6834\n","Epoch: 187 | Train loss: 462.8877 | Test loss: 466.2265\n","Epoch: 188 | Train loss: 462.2002 | Test loss: 466.5103\n","Epoch: 189 | Train loss: 461.8454 | Test loss: 465.8160\n","Epoch: 190 | Train loss: 460.9306 | Test loss: 464.5364\n","Epoch: 191 | Train loss: 460.1839 | Test loss: 464.2484\n","Epoch: 192 | Train loss: 459.8649 | Test loss: 464.3107\n","Epoch: 193 | Train loss: 458.9766 | Test loss: 462.9830\n","Epoch: 194 | Train loss: 458.3017 | Test loss: 462.9416\n","Epoch: 195 | Train loss: 457.9781 | Test loss: 462.5195\n","Epoch: 196 | Train loss: 457.2379 | Test loss: 461.8337\n","Epoch: 197 | Train loss: 456.6088 | Test loss: 461.3281\n","Epoch: 198 | Train loss: 456.1189 | Test loss: 461.2599\n","Epoch: 199 | Train loss: 455.4064 | Test loss: 458.8445\n","Epoch: 200 | Train loss: 454.4826 | Test loss: 458.3899\n","Epoch: 201 | Train loss: 453.7288 | Test loss: 457.9236\n","Epoch: 202 | Train loss: 453.1742 | Test loss: 456.9360\n","Epoch: 203 | Train loss: 452.3387 | Test loss: 456.0706\n","Epoch: 204 | Train loss: 451.7726 | Test loss: 455.7780\n","Epoch: 205 | Train loss: 450.8242 | Test loss: 456.9551\n","Epoch: 206 | Train loss: 450.8711 | Test loss: 456.3908\n","Epoch: 207 | Train loss: 449.9648 | Test loss: 453.8673\n","Epoch: 208 | Train loss: 449.6620 | Test loss: 453.9704\n","Epoch: 209 | Train loss: 449.2993 | Test loss: 453.8733\n","Epoch: 210 | Train loss: 448.5817 | Test loss: 452.7405\n","Epoch: 211 | Train loss: 448.3302 | Test loss: 452.3388\n","Epoch: 212 | Train loss: 448.6389 | Test loss: 454.2162\n","Epoch: 213 | Train loss: 447.7682 | Test loss: 452.1180\n","Epoch: 214 | Train loss: 447.5856 | Test loss: 450.9256\n","Epoch: 215 | Train loss: 447.4529 | Test loss: 451.1333\n","Epoch: 216 | Train loss: 446.9241 | Test loss: 451.2903\n","Epoch: 217 | Train loss: 446.0878 | Test loss: 450.8032\n","Epoch: 218 | Train loss: 445.8427 | Test loss: 449.7262\n","Epoch: 219 | Train loss: 445.8896 | Test loss: 449.4439\n","Epoch: 220 | Train loss: 445.6995 | Test loss: 451.4111\n","Epoch: 221 | Train loss: 445.4148 | Test loss: 450.3655\n","Epoch: 222 | Train loss: 445.2100 | Test loss: 450.9756\n","Epoch: 223 | Train loss: 444.6008 | Test loss: 449.9725\n","Epoch: 224 | Train loss: 444.8483 | Test loss: 448.5936\n","Epoch: 225 | Train loss: 444.2342 | Test loss: 449.1685\n","Epoch: 226 | Train loss: 443.9018 | Test loss: 449.6379\n","Epoch: 227 | Train loss: 443.5015 | Test loss: 448.5552\n","Epoch: 228 | Train loss: 443.6144 | Test loss: 447.4108\n","Epoch: 229 | Train loss: 442.8588 | Test loss: 448.2145\n","Epoch: 230 | Train loss: 443.0586 | Test loss: 448.8532\n","Epoch: 231 | Train loss: 443.1056 | Test loss: 446.8785\n","Epoch: 232 | Train loss: 442.2265 | Test loss: 446.4848\n","Epoch: 233 | Train loss: 442.0703 | Test loss: 446.8004\n","Epoch: 234 | Train loss: 441.6253 | Test loss: 445.8205\n","Epoch: 235 | Train loss: 441.6837 | Test loss: 445.0269\n","Epoch: 236 | Train loss: 441.2305 | Test loss: 445.8923\n","Epoch: 237 | Train loss: 441.1873 | Test loss: 445.4960\n","Epoch: 238 | Train loss: 440.6839 | Test loss: 444.7626\n","Epoch: 239 | Train loss: 440.4470 | Test loss: 446.5680\n","Epoch: 240 | Train loss: 440.2002 | Test loss: 446.0720\n","Epoch: 241 | Train loss: 439.5037 | Test loss: 443.8737\n","Epoch: 242 | Train loss: 439.8549 | Test loss: 444.5000\n","Epoch: 243 | Train loss: 438.9171 | Test loss: 443.6175\n","Epoch: 244 | Train loss: 438.5140 | Test loss: 444.3917\n","Epoch: 245 | Train loss: 438.2852 | Test loss: 442.8754\n","Epoch: 246 | Train loss: 438.0412 | Test loss: 442.3529\n","Epoch: 247 | Train loss: 437.4357 | Test loss: 441.5925\n","Epoch: 248 | Train loss: 437.6995 | Test loss: 442.9352\n","Epoch: 249 | Train loss: 437.5453 | Test loss: 441.6417\n","Epoch: 250 | Train loss: 436.8068 | Test loss: 440.8043\n","Epoch: 251 | Train loss: 436.9779 | Test loss: 440.5250\n","Epoch: 252 | Train loss: 436.3221 | Test loss: 440.1048\n","Epoch: 253 | Train loss: 435.4381 | Test loss: 440.8788\n","Epoch: 254 | Train loss: 436.1282 | Test loss: 440.0935\n","Epoch: 255 | Train loss: 435.8229 | Test loss: 438.2869\n","Epoch: 256 | Train loss: 436.3691 | Test loss: 439.7460\n","Epoch: 257 | Train loss: 435.8619 | Test loss: 440.9743\n","Epoch: 258 | Train loss: 435.6836 | Test loss: 439.8280\n","Epoch: 259 | Train loss: 434.9471 | Test loss: 441.4713\n","Epoch: 260 | Train loss: 434.8945 | Test loss: 439.5782\n","Epoch: 261 | Train loss: 435.0828 | Test loss: 439.1454\n","Epoch: 262 | Train loss: 435.2587 | Test loss: 439.3334\n","Epoch: 263 | Train loss: 434.6795 | Test loss: 438.7735\n","Epoch: 264 | Train loss: 434.1427 | Test loss: 438.7718\n","Epoch: 265 | Train loss: 434.1436 | Test loss: 440.9125\n","Epoch: 266 | Train loss: 434.2831 | Test loss: 440.1796\n","Epoch: 267 | Train loss: 434.3345 | Test loss: 437.2828\n","Epoch: 268 | Train loss: 433.7163 | Test loss: 437.4480\n","Epoch: 269 | Train loss: 433.9545 | Test loss: 438.1045\n","Epoch: 270 | Train loss: 433.5593 | Test loss: 438.3415\n","Epoch: 271 | Train loss: 433.7762 | Test loss: 439.5209\n","Epoch: 272 | Train loss: 433.8890 | Test loss: 438.2524\n","Epoch: 273 | Train loss: 433.4847 | Test loss: 437.0770\n","Epoch: 274 | Train loss: 433.4355 | Test loss: 437.7301\n","Epoch: 275 | Train loss: 433.3843 | Test loss: 438.2298\n","Epoch: 276 | Train loss: 432.9518 | Test loss: 437.5818\n","Epoch: 277 | Train loss: 432.8646 | Test loss: 437.5971\n","Epoch: 278 | Train loss: 432.9928 | Test loss: 435.1678\n","Epoch: 279 | Train loss: 432.5200 | Test loss: 437.8281\n","Epoch: 280 | Train loss: 432.9841 | Test loss: 438.1801\n","Epoch: 281 | Train loss: 432.3843 | Test loss: 437.1209\n","Epoch: 282 | Train loss: 432.5930 | Test loss: 436.0743\n","Epoch: 283 | Train loss: 432.5350 | Test loss: 436.6036\n","Epoch: 284 | Train loss: 431.6465 | Test loss: 436.8469\n","Epoch: 285 | Train loss: 432.1592 | Test loss: 438.3626\n","Epoch: 286 | Train loss: 431.9701 | Test loss: 438.2943\n","Epoch: 287 | Train loss: 431.1785 | Test loss: 436.8174\n","Epoch: 288 | Train loss: 431.2268 | Test loss: 437.2373\n","Epoch: 289 | Train loss: 431.3000 | Test loss: 436.9398\n","Epoch: 290 | Train loss: 431.0211 | Test loss: 436.2630\n","Epoch: 291 | Train loss: 430.5070 | Test loss: 434.9345\n","Epoch: 292 | Train loss: 430.4531 | Test loss: 434.4211\n","Epoch: 293 | Train loss: 430.0904 | Test loss: 435.7157\n","Epoch: 294 | Train loss: 430.1238 | Test loss: 434.4801\n","Epoch: 295 | Train loss: 429.7985 | Test loss: 434.8905\n","Epoch: 296 | Train loss: 429.3151 | Test loss: 435.3847\n","Epoch: 297 | Train loss: 429.7831 | Test loss: 434.2021\n","Epoch: 298 | Train loss: 429.1349 | Test loss: 433.7593\n","Epoch: 299 | Train loss: 428.9166 | Test loss: 432.1744\n","Epoch: 300 | Train loss: 429.1039 | Test loss: 434.2283\n","Epoch: 301 | Train loss: 427.8913 | Test loss: 433.4345\n","Epoch: 302 | Train loss: 428.1418 | Test loss: 431.4371\n","Epoch: 303 | Train loss: 427.1634 | Test loss: 431.0778\n","Epoch: 304 | Train loss: 427.1452 | Test loss: 432.0947\n","Epoch: 305 | Train loss: 427.3654 | Test loss: 433.8343\n","Epoch: 306 | Train loss: 426.4387 | Test loss: 432.7542\n","Epoch: 307 | Train loss: 426.2089 | Test loss: 428.6382\n","Epoch: 308 | Train loss: 426.1902 | Test loss: 430.3379\n","Epoch: 309 | Train loss: 426.2780 | Test loss: 429.5104\n","Epoch: 310 | Train loss: 425.7891 | Test loss: 430.3206\n","Epoch: 311 | Train loss: 425.6887 | Test loss: 430.3887\n","Epoch: 312 | Train loss: 425.3216 | Test loss: 430.8771\n","Epoch: 313 | Train loss: 424.7300 | Test loss: 430.4084\n","Epoch: 314 | Train loss: 425.1656 | Test loss: 429.9643\n","Epoch: 315 | Train loss: 424.5547 | Test loss: 429.2822\n","Epoch: 316 | Train loss: 424.2030 | Test loss: 428.4444\n","Epoch: 317 | Train loss: 424.4275 | Test loss: 428.3427\n","Epoch: 318 | Train loss: 424.3923 | Test loss: 427.6347\n","Epoch: 319 | Train loss: 424.0693 | Test loss: 427.5984\n","Epoch: 320 | Train loss: 423.8225 | Test loss: 425.9987\n","Epoch: 321 | Train loss: 423.7901 | Test loss: 429.1727\n","Epoch: 322 | Train loss: 423.0221 | Test loss: 429.8683\n","Epoch: 323 | Train loss: 423.2412 | Test loss: 427.5451\n","Epoch: 324 | Train loss: 423.0371 | Test loss: 428.3380\n","Epoch: 325 | Train loss: 423.1728 | Test loss: 428.4673\n","Epoch: 326 | Train loss: 422.9716 | Test loss: 427.0294\n","Epoch: 327 | Train loss: 422.7384 | Test loss: 426.7626\n","Epoch: 328 | Train loss: 422.0935 | Test loss: 428.6298\n","Epoch: 329 | Train loss: 422.3656 | Test loss: 428.1280\n","Epoch: 330 | Train loss: 422.5865 | Test loss: 428.2048\n","Epoch: 331 | Train loss: 422.2851 | Test loss: 426.9903\n","Epoch: 332 | Train loss: 422.2502 | Test loss: 426.2066\n","Epoch: 333 | Train loss: 422.0562 | Test loss: 425.7754\n","Epoch: 334 | Train loss: 422.0087 | Test loss: 426.6866\n","Epoch: 335 | Train loss: 421.9395 | Test loss: 427.5715\n","Epoch: 336 | Train loss: 421.1963 | Test loss: 425.5107\n","Epoch: 337 | Train loss: 421.8974 | Test loss: 427.9225\n","Epoch: 338 | Train loss: 421.6763 | Test loss: 427.8365\n","Epoch: 339 | Train loss: 421.2598 | Test loss: 425.5438\n","Epoch: 340 | Train loss: 421.4551 | Test loss: 426.6441\n","Epoch: 341 | Train loss: 421.2313 | Test loss: 425.0427\n","Epoch: 342 | Train loss: 421.8564 | Test loss: 425.5938\n","Epoch: 343 | Train loss: 420.9624 | Test loss: 426.4607\n","Epoch: 344 | Train loss: 420.4753 | Test loss: 425.5176\n","Epoch: 345 | Train loss: 420.8767 | Test loss: 425.9510\n","Epoch: 346 | Train loss: 421.3639 | Test loss: 426.1443\n","Epoch: 347 | Train loss: 420.7628 | Test loss: 425.0894\n","Epoch: 348 | Train loss: 420.2791 | Test loss: 425.2754\n","Epoch: 349 | Train loss: 420.5904 | Test loss: 425.9317\n","Epoch: 350 | Train loss: 419.4772 | Test loss: 426.3734\n","Epoch: 351 | Train loss: 419.9003 | Test loss: 423.8939\n","Epoch: 352 | Train loss: 419.6460 | Test loss: 425.6811\n","Epoch: 353 | Train loss: 419.8110 | Test loss: 424.1805\n","Epoch: 354 | Train loss: 419.8988 | Test loss: 424.9752\n","Epoch: 355 | Train loss: 419.4250 | Test loss: 424.6669\n","Epoch: 356 | Train loss: 419.2716 | Test loss: 424.5470\n","Epoch: 357 | Train loss: 419.5766 | Test loss: 424.9217\n","Epoch: 358 | Train loss: 419.6400 | Test loss: 425.3894\n","Epoch: 359 | Train loss: 419.4242 | Test loss: 424.2733\n","Epoch: 360 | Train loss: 418.4044 | Test loss: 424.1586\n","Epoch: 361 | Train loss: 418.8868 | Test loss: 424.7548\n","Epoch: 362 | Train loss: 418.8847 | Test loss: 423.2904\n","Epoch: 363 | Train loss: 419.5687 | Test loss: 421.8275\n","Epoch: 364 | Train loss: 418.4434 | Test loss: 423.6215\n","Epoch: 365 | Train loss: 418.1377 | Test loss: 424.7066\n","Epoch: 366 | Train loss: 417.9858 | Test loss: 424.2181\n","Epoch: 367 | Train loss: 418.1104 | Test loss: 423.3145\n","Epoch: 368 | Train loss: 418.3727 | Test loss: 421.4432\n","Epoch: 369 | Train loss: 417.6250 | Test loss: 421.9685\n","Epoch: 370 | Train loss: 418.0637 | Test loss: 424.2088\n","Epoch: 371 | Train loss: 417.4407 | Test loss: 422.8247\n","Epoch: 372 | Train loss: 417.4116 | Test loss: 423.1777\n","Epoch: 373 | Train loss: 417.7421 | Test loss: 422.8289\n","Epoch: 374 | Train loss: 418.2045 | Test loss: 422.5787\n","Epoch: 375 | Train loss: 417.3512 | Test loss: 421.9810\n","Epoch: 376 | Train loss: 416.8148 | Test loss: 422.9606\n","Epoch: 377 | Train loss: 416.2685 | Test loss: 421.1219\n","Epoch: 378 | Train loss: 417.0146 | Test loss: 421.7932\n","Epoch: 379 | Train loss: 416.8255 | Test loss: 421.7427\n","Epoch: 380 | Train loss: 416.8249 | Test loss: 421.5240\n","Epoch: 381 | Train loss: 416.8133 | Test loss: 422.4527\n","Epoch: 382 | Train loss: 416.8906 | Test loss: 420.7525\n","Epoch: 383 | Train loss: 416.4660 | Test loss: 422.2505\n","Epoch: 384 | Train loss: 416.6387 | Test loss: 422.0756\n","Epoch: 385 | Train loss: 416.5172 | Test loss: 420.8160\n","Epoch: 386 | Train loss: 416.3692 | Test loss: 421.1033\n","Epoch: 387 | Train loss: 416.8973 | Test loss: 420.5184\n","Epoch: 388 | Train loss: 416.0542 | Test loss: 422.3473\n","Epoch: 389 | Train loss: 414.9845 | Test loss: 421.1097\n","Epoch: 390 | Train loss: 416.1839 | Test loss: 421.6270\n","Epoch: 391 | Train loss: 416.1053 | Test loss: 421.9022\n","Epoch: 392 | Train loss: 416.5108 | Test loss: 420.2903\n","Epoch: 393 | Train loss: 415.6946 | Test loss: 421.7892\n","Epoch: 394 | Train loss: 415.8410 | Test loss: 421.2951\n","Epoch: 395 | Train loss: 415.1448 | Test loss: 421.0689\n","Epoch: 396 | Train loss: 415.4609 | Test loss: 419.6842\n","Epoch: 397 | Train loss: 415.6206 | Test loss: 422.4926\n","Epoch: 398 | Train loss: 415.8181 | Test loss: 419.0144\n","Epoch: 399 | Train loss: 416.0449 | Test loss: 422.2600\n","Total training time: 202.181 seconds\n"]}]},{"cell_type":"code","source":["ans=None\n","model_main.to(\"cpu\")\n","model_main.eval()\n","with torch.inference_mode():\n","  for batch, tp in enumerate(test_dataloader_simple):\n","    # print(tp.shape)\n","    X=tp[:,:-2]\n","    gt=tp[:,-1]\n","\n","\n","\n","    X.to(device)\n","    # print(X.shape)\n","    txxx=model_main.get_feature(X)\n","    gt = torch.unsqueeze(gt, dim=-1)\n","    txxx=torch.cat((txxx,gt), axis=1)\n","    # print(txxx.shape)\n","    if ans is None:\n","      ans=txxx\n","    else:\n","      ans=torch.cat((ans,txxx), axis=0)\n","\n","# torch.save(ans, \"iemrvector_VAE.pt\")"],"metadata":{"id":"9ai38bJQ1H5e","executionInfo":{"status":"ok","timestamp":1738875754168,"user_tz":300,"elapsed":334,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["torch.save(ans, \"iemeapa_betaVAE.pt\")"],"metadata":{"id":"xx-Hkz7zveUT","executionInfo":{"status":"ok","timestamp":1738872389121,"user_tz":300,"elapsed":946,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["torch.save(ans, \"iemeapa_infoVAE2.pt\")"],"metadata":{"id":"DLspjr171OzU","executionInfo":{"status":"ok","timestamp":1738875762088,"user_tz":300,"elapsed":336,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"unPUUePM5Phd"}},{"cell_type":"markdown","source":["INFOVAE"],"metadata":{"id":"uWEqEstN5Pj3"}},{"cell_type":"code","source":["import torch.nn as nn\n","class info_VAE(nn.Module):\n","    def __init__(self, input_dim=256, output_dim=256, num_domains=2, hidden_dim=384,emoclassnum=5,spkclassnum=10):\n","        super().__init__()\n","        self.output_dim=output_dim\n","        layers = []\n","        layers += [nn.Linear(input_dim, hidden_dim)]\n","        layers += [nn.ReLU()]\n","        for _ in range(2):\n","            layers += [nn.Linear(hidden_dim, hidden_dim)]\n","            layers += [nn.LayerNorm(hidden_dim)]\n","        layers +=[nn.Linear(hidden_dim, output_dim)]\n","        layers +=[nn.LayerNorm(output_dim)]\n","\n","\n","        self.shared = nn.Sequential(*layers)\n","\n","        # self.speakerencoder=nn.Sequential(*[nn.Linear(hidden_dim, hidden_dim),\n","        #                                     nn.LayerNorm(hidden_dim),\n","        #                                     nn.Linear(hidden_dim, output_dim * 2),\n","        #                                     nn.LayerNorm(hidden_dim)])\n","\n","        # self.emotionencoder=nn.Sequential(*[nn.Linear(hidden_dim, hidden_dim),\n","        #                                     nn.LayerNorm(hidden_dim),\n","        #                                     nn.Linear(hidden_dim, output_dim * 2),\n","        #                                     nn.LayerNorm(hidden_dim)])\n","\n","        self.decoder=nn.Sequential(*[nn.Linear(output_dim, hidden_dim),\n","                                      nn.ReLU(),\n","                                      nn.Linear(hidden_dim, input_dim)]\n","                                   )\n","\n","        # self.emocls = nn.Sequential(*[nn.Linear(output_dim * 2, emoclassnum)])\n","\n","        # self.spkcls = nn.Sequential(*[nn.Linear(output_dim * 2, spkclassnum)])\n","\n","\n","\n","        '''\n","        self.unshared = nn.ModuleList()\n","        for _ in range(num_domains):\n","            self.unshared += [nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n","                                            nn.ReLU(),\n","                                            nn.Linear(hidden_dim, hidden_dim),\n","                                            nn.ReLU(),\n","                                            nn.Linear(hidden_dim, style_dim))]\n","        '''\n","    def forward(self, x):\n","        h=self.shared(x)\n","\n","        # emo_mu_logstd=self.emotionencoder(h)\n","        # spk_mu_logstd=self.speakerencoder(h)\n","\n","        # emo_mu, emo_log_std = torch.split(emo_mu_logstd,self.output_dim, dim=1)\n","        # spk_mu, spk_log_std = torch.split(spk_mu_logstd,self.output_dim, dim=1)\n","\n","        # mu, log_std = torch.split(h,self.output_dim, dim=1)\n","\n","        # z = sample(mu, log_std)\n","        z= torch.randn(h.shape[0], h.shape[1])\n","\n","        x_hat = self.decoder(h)\n","\n","        return {'x_hat': x_hat, 'z': z, 'h':h}\n","\n","\n","    def get_feature(self,x):\n","        h=self.shared(x)\n","\n","        # spk_mu_logstd=self.speakerencoder(h)\n","\n","        return h\n","    '''\n","    def get_emo(self,x):\n","        h=self.shared(x)\n","        emo_mu, emo_log_std = self.emotionencoder(h)\n","        dec_in = torch.cat([emo_mu, emo_log_std], dim=1)\n","        emoresult=self.emocls(dec_in)\n","\n","    def get_spk(self,x):\n","        h=self.shared(x)\n","        spk_mu, spk_log_std = self.speakerencoder(h)\n","        dec_in = torch.cat([spk_mu, spk_log_std], dim=1)\n","        spkresult=self.spkcls(dec_in)\n","    '''\n","    def loss_fn(self, outputs, x_gt):\n","        x_hat = outputs['x_hat']\n","        # mu = outputs['mu']\n","        # log_std = outputs['log_std']\n","        h=outputs['h']\n","        z=outputs['z']\n","        # spk_mu = outputs['spk_mu']\n","        # spk_log_std = outputs['spk_log_std']\n","        # emoresult=outputs['emocls']\n","        # spkresult=outputs['spkcls']\n","\n","        # kl = kl_divergence(mu, log_std)\n","        # spk_kl = kl_divergence(spk_mu, spk_log_std)\n","        mmd = compute_mmd(h, z)\n","        # mi_loss=mutual_information(sample(emo_mu, emo_log_std),sample(spk_mu, spk_log_std))\n","        loss_2 = nn.MSELoss()\n","        nll = 0.5*loss_1(x_hat, x_gt) + 0.5*loss_2(x_hat, x_gt)\n","\n","        # emo_cls=loss_cls(emoresult, emo_gt)\n","        # spk_cls=loss_cls(spkresult,spk_gt)\n","\n","        return nll,mmd\n","\n","# Create train_step()\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               #loss_fn: torch.nn.Module,\n","               optimizer:torch.optim.Optimizer,\n","               device=device):\n","  # Put the model in train mode\n","  model.train()\n","\n","  # Setup train loss and train accuracy values\n","  train_loss = 0\n","\n","  # Loop through data loader data batches\n","  for batch, tp in enumerate(dataloader):\n","    # Send data to the target device\n","    X=tp[:,:-2]\n","\n","    X_gt=X\n","\n","    #y=y.type(torch.LongTensor)\n","    X, X_gt = X.to(device), X_gt.to(device)\n","\n","    # 1. Forward pass\n","    y_pred = model(X) # output model logits\n","    #print(y_pred['spkcls'].shape)\n","    #y_pred = torch.softmax(y_pred,dim=1)\n","    #print(y_pred,y)\n","    # 2. Calculate the loss\n","    #y_pred=y_pred.type(torch.float32)\n","\n","    nll,mmd = model.loss_fn(y_pred, X_gt)\n","    loss=nll + mmd\n","    train_loss += loss.item()\n","\n","    # 3. Optimizer zero grad\n","    optimizer.zero_grad()\n","\n","    # 4. Loss backward\n","    loss.backward()\n","\n","    # 5. Optimizer step\n","    optimizer.step()\n","\n","    # Calculate accuracy metric\n","    # y_pred_spkclass = torch.argmax(torch.softmax(y_pred['spkcls'], dim=1), dim=1)\n","    # spk_train_acc += (y_pred_spkclass==spk_gt).sum().item()/len(y_pred['spkcls'])\n","\n","    # y_pred_emoclass = torch.argmax(torch.softmax(y_pred['emocls'], dim=1), dim=1)\n","    # emo_train_acc += (y_pred_emoclass==emo_gt).sum().item()/len(y_pred['emocls'])\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  train_loss = train_loss / len(dataloader)\n","  # spk_train_acc = spk_train_acc / len(dataloader)\n","  # emo_train_acc = emo_train_acc / len(dataloader)\n","  return train_loss\n","\n","# Create a test step\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              #loss_fn: torch.nn.Module,\n","              device=device):\n","  # Put model in eval mode\n","  model.eval()\n","\n","  # Setup test loss and test accuracy values\n","  test_loss  = 0\n","\n","  # Turn on inference mode\n","  with torch.inference_mode():\n","    # Loop through DataLoader batches\n","    for batch, tp in enumerate(dataloader):\n","      # Send data to the target device\n","      X=tp[:,:-2]\n","\n","      X_gt=X\n","\n","      #y=y.type(torch.LongTensor)\n","      X, X_gt = X.to(device), X_gt.to(device)\n","\n","      # 1. Forward pass\n","      y_pred = model(X) # output model logits\n","\n","      #y_pred = torch.softmax(y_pred,dim=1)\n","      #print(y_pred,y)\n","      # 2. Calculate the loss\n","      #y_pred=y_pred.type(torch.float32)\n","      nll,mmd = model.loss_fn(y_pred, X_gt)\n","      loss=nll + mmd\n","      test_loss += loss.item()\n","\n","      # Calculate accuracy metric\n","      # y_pred_spkclass = torch.argmax(torch.softmax(y_pred['spkcls'], dim=1), dim=1)\n","      # spk_test_acc += (y_pred_spkclass==spk_gt).sum().item()/len(y_pred['spkcls'])\n","\n","      # y_pred_emoclass = torch.argmax(torch.softmax(y_pred['emocls'], dim=1), dim=1)\n","      # emo_test_acc += (y_pred_emoclass==emo_gt).sum().item()/len(y_pred['emocls'])\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","\n","  return test_loss\n","\n","\n","\n","from tqdm.auto import tqdm\n","\n","# 1. Create a train function that takes in various model parameters + optimizer + dataloaders + loss function\n","def train(model: torch.nn.Module,\n","          train_dataloader,\n","          test_dataloader,\n","          optimizer,\n","          #loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n","          epochs: int = 5,\n","          device=device):\n","\n","  # 2. Create empty results dictionary\n","  results = {\"train_loss\": [],\n","             \"spk_train_acc\": [],\n","             \"emo_train_acc\": [],\n","             \"test_loss\": [],\n","             \"spk_test_acc\": [],\n","             \"emo_test_acc\": [],}\n","\n","  # 3. Loop through training and testing steps for a number of epochs\n","  for epoch in tqdm(range(epochs)):\n","    train_loss = train_step(model=model,\n","                            dataloader=train_dataloader,\n","                            #loss_fn=loss_fn,\n","                            optimizer=optimizer,\n","                            device=device)\n","    test_loss= test_step(model=model,\n","                        dataloader=test_dataloader,\n","                        #loss_fn=loss_fn,\n","                        device=device)\n","\n","    # 4. Print out what's happening\n","    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Test loss: {test_loss:.4f}\")\n","\n","    # 5. Update results dictionary\n","    results[\"train_loss\"].append(train_loss)\n","\n","  # 6. Return the filled results at the end of the epochs\n","  return results\n"],"metadata":{"id":"9edvOCkX5Rb1","executionInfo":{"status":"ok","timestamp":1738874797968,"user_tz":300,"elapsed":989,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["def compute_kernel(x, y):\n","    x_size = x.size(0)\n","    y_size = y.size(0)\n","    dim = x.size(1)\n","    x = x.unsqueeze(1) # (x_size, 1, dim)\n","    y = y.unsqueeze(0) # (1, y_size, dim)\n","    tiled_x = x.expand(x_size, y_size, dim)\n","    tiled_y = y.expand(x_size, y_size, dim)\n","    kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)\n","    return torch.exp(-kernel_input) # (x_size, y_size)\n","\n","def compute_mmd(x, y):\n","    x_kernel = compute_kernel(x, x)\n","    y_kernel = compute_kernel(y, y)\n","    x_kernel.to(device), y_kernel.to(device)\n","    xy_kernel = compute_kernel(x, y)\n","    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()\n","    return mmd"],"metadata":{"id":"P6monMuX7fOP","executionInfo":{"status":"ok","timestamp":1738872420011,"user_tz":300,"elapsed":327,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["\n","# Set random seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","# Set number of epochs\n","NUM_EPOCHS = 400\n","beta=4.0\n","\n","# Recreate an instance of TinyVGG\n","'''\n","model_0 = TinyVGG(input_shape=3, # number of color channels of our target images\n","                  hidden_units=10,\n","                  output_shape=len(train_data.classes)).to(device)\n","'''\n","model_main=info_VAE(input_dim=192,\n","                        hidden_dim=64)\n","model_main.to(device)\n","# Setup loss function and optimizer\n","\n","optimizer = torch.optim.Adam(params=model_main.parameters(),\n","                             lr=0.0001)\n","\n","# Start the timer\n","from timeit import default_timer as timer\n","start_time = timer()\n","\n","# Train model_0\n","model_0_results = train(model=model_main,\n","                        train_dataloader=train_dataloader_simple,\n","                        test_dataloader=test_dataloader_simple,\n","                        optimizer=optimizer,\n","                        #loss_fn=loss_fn(),\n","                        epochs=NUM_EPOCHS)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7fa697850dee445ebf24992ba9cc46e9","7c21f8e2773a4a7fb7f2fbbfebb8f907","fa4643b78c32461d83b0eb1bd836d197","072399c78b5145eeab04ac5ba8521b85","580971feadda498a9c70e1c2556d78d8","6c4b75a4fd014cf4881ba0382b09d0da","d9d5c98fd23147a49de448c2c498a110","d460975df7224d5cb79260775f47d883","ef7ce8fe162e4622ab85a087571aeda5","73018042acc24943bc56c4fdfbf48e3a","47306c6512384726bfb9e97d3f47eb7e"]},"id":"_OIgQQkGPcCv","executionInfo":{"status":"ok","timestamp":1738875736456,"user_tz":300,"elapsed":930689,"user":{"displayName":"Chaohao Lin","userId":"02535404984035233613"}},"outputId":"6e4c3e39-0c91-4c73-baa5-87e2365366f2"},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/400 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fa697850dee445ebf24992ba9cc46e9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Train loss: 458.0009 | Test loss: 412.4364\n","Epoch: 1 | Train loss: 376.3608 | Test loss: 345.2039\n","Epoch: 2 | Train loss: 320.4447 | Test loss: 302.1810\n","Epoch: 3 | Train loss: 289.6821 | Test loss: 280.1376\n","Epoch: 4 | Train loss: 271.3905 | Test loss: 265.3852\n","Epoch: 5 | Train loss: 260.3933 | Test loss: 257.2403\n","Epoch: 6 | Train loss: 253.1629 | Test loss: 250.6230\n","Epoch: 7 | Train loss: 246.4671 | Test loss: 244.1321\n","Epoch: 8 | Train loss: 240.2764 | Test loss: 238.1471\n","Epoch: 9 | Train loss: 234.3004 | Test loss: 232.4162\n","Epoch: 10 | Train loss: 228.5230 | Test loss: 226.7806\n","Epoch: 11 | Train loss: 222.9209 | Test loss: 221.0444\n","Epoch: 12 | Train loss: 217.1958 | Test loss: 215.6996\n","Epoch: 13 | Train loss: 212.1139 | Test loss: 210.7946\n","Epoch: 14 | Train loss: 207.5037 | Test loss: 206.4530\n","Epoch: 15 | Train loss: 203.3943 | Test loss: 202.5369\n","Epoch: 16 | Train loss: 199.5136 | Test loss: 198.8242\n","Epoch: 17 | Train loss: 195.7700 | Test loss: 195.2823\n","Epoch: 18 | Train loss: 192.3009 | Test loss: 192.1389\n","Epoch: 19 | Train loss: 189.0791 | Test loss: 189.0813\n","Epoch: 20 | Train loss: 186.0221 | Test loss: 185.9992\n","Epoch: 21 | Train loss: 183.0197 | Test loss: 183.1745\n","Epoch: 22 | Train loss: 180.2263 | Test loss: 180.5772\n","Epoch: 23 | Train loss: 177.6544 | Test loss: 178.0377\n","Epoch: 24 | Train loss: 175.2652 | Test loss: 175.6972\n","Epoch: 25 | Train loss: 172.8096 | Test loss: 173.1415\n","Epoch: 26 | Train loss: 170.3720 | Test loss: 170.7278\n","Epoch: 27 | Train loss: 167.9801 | Test loss: 168.4792\n","Epoch: 28 | Train loss: 165.8476 | Test loss: 166.5480\n","Epoch: 29 | Train loss: 164.1270 | Test loss: 164.7547\n","Epoch: 30 | Train loss: 162.5153 | Test loss: 163.2524\n","Epoch: 31 | Train loss: 161.0967 | Test loss: 162.0310\n","Epoch: 32 | Train loss: 159.8883 | Test loss: 160.8742\n","Epoch: 33 | Train loss: 158.7850 | Test loss: 159.8185\n","Epoch: 34 | Train loss: 157.7163 | Test loss: 158.7505\n","Epoch: 35 | Train loss: 156.6229 | Test loss: 157.4699\n","Epoch: 36 | Train loss: 155.2913 | Test loss: 156.4038\n","Epoch: 37 | Train loss: 154.2440 | Test loss: 155.2506\n","Epoch: 38 | Train loss: 153.1039 | Test loss: 154.2743\n","Epoch: 39 | Train loss: 152.0408 | Test loss: 153.2265\n","Epoch: 40 | Train loss: 151.0413 | Test loss: 152.3392\n","Epoch: 41 | Train loss: 150.2472 | Test loss: 151.5362\n","Epoch: 42 | Train loss: 149.4659 | Test loss: 150.7729\n","Epoch: 43 | Train loss: 148.8222 | Test loss: 150.1064\n","Epoch: 44 | Train loss: 148.1361 | Test loss: 149.3713\n","Epoch: 45 | Train loss: 147.3576 | Test loss: 148.5147\n","Epoch: 46 | Train loss: 146.6279 | Test loss: 147.9276\n","Epoch: 47 | Train loss: 145.9800 | Test loss: 147.2550\n","Epoch: 48 | Train loss: 145.4869 | Test loss: 146.7098\n","Epoch: 49 | Train loss: 144.7526 | Test loss: 145.9464\n","Epoch: 50 | Train loss: 144.2142 | Test loss: 145.4578\n","Epoch: 51 | Train loss: 143.6817 | Test loss: 144.9433\n","Epoch: 52 | Train loss: 143.1574 | Test loss: 144.3988\n","Epoch: 53 | Train loss: 142.6052 | Test loss: 143.8068\n","Epoch: 54 | Train loss: 142.0171 | Test loss: 143.1090\n","Epoch: 55 | Train loss: 141.4629 | Test loss: 142.6837\n","Epoch: 56 | Train loss: 140.9975 | Test loss: 142.1677\n","Epoch: 57 | Train loss: 140.4646 | Test loss: 141.5530\n","Epoch: 58 | Train loss: 139.9833 | Test loss: 141.0197\n","Epoch: 59 | Train loss: 139.3614 | Test loss: 140.4056\n","Epoch: 60 | Train loss: 138.6204 | Test loss: 139.6886\n","Epoch: 61 | Train loss: 137.8830 | Test loss: 138.9146\n","Epoch: 62 | Train loss: 137.1294 | Test loss: 138.1556\n","Epoch: 63 | Train loss: 136.4355 | Test loss: 137.5752\n","Epoch: 64 | Train loss: 135.9194 | Test loss: 137.0992\n","Epoch: 65 | Train loss: 135.5093 | Test loss: 136.6284\n","Epoch: 66 | Train loss: 135.0494 | Test loss: 136.1656\n","Epoch: 67 | Train loss: 134.6936 | Test loss: 135.8536\n","Epoch: 68 | Train loss: 134.4394 | Test loss: 135.4975\n","Epoch: 69 | Train loss: 134.0765 | Test loss: 135.2304\n","Epoch: 70 | Train loss: 133.7147 | Test loss: 134.9392\n","Epoch: 71 | Train loss: 133.4812 | Test loss: 134.6163\n","Epoch: 72 | Train loss: 133.0559 | Test loss: 133.9951\n","Epoch: 73 | Train loss: 132.5827 | Test loss: 133.6884\n","Epoch: 74 | Train loss: 132.1858 | Test loss: 133.3145\n","Epoch: 75 | Train loss: 131.8041 | Test loss: 133.0220\n","Epoch: 76 | Train loss: 131.5485 | Test loss: 132.8018\n","Epoch: 77 | Train loss: 131.3229 | Test loss: 132.6114\n","Epoch: 78 | Train loss: 131.1039 | Test loss: 132.3202\n","Epoch: 79 | Train loss: 130.9412 | Test loss: 132.1420\n","Epoch: 80 | Train loss: 130.7109 | Test loss: 131.9132\n","Epoch: 81 | Train loss: 130.5524 | Test loss: 131.7825\n","Epoch: 82 | Train loss: 130.4197 | Test loss: 131.5965\n","Epoch: 83 | Train loss: 130.2332 | Test loss: 131.4501\n","Epoch: 84 | Train loss: 130.0663 | Test loss: 131.3240\n","Epoch: 85 | Train loss: 129.8740 | Test loss: 131.1812\n","Epoch: 86 | Train loss: 129.6315 | Test loss: 130.8469\n","Epoch: 87 | Train loss: 129.3720 | Test loss: 130.5768\n","Epoch: 88 | Train loss: 128.9546 | Test loss: 130.1950\n","Epoch: 89 | Train loss: 128.6438 | Test loss: 130.0197\n","Epoch: 90 | Train loss: 128.5051 | Test loss: 129.8859\n","Epoch: 91 | Train loss: 128.3046 | Test loss: 129.6437\n","Epoch: 92 | Train loss: 128.1475 | Test loss: 129.4658\n","Epoch: 93 | Train loss: 128.0023 | Test loss: 129.3592\n","Epoch: 94 | Train loss: 127.8952 | Test loss: 129.2841\n","Epoch: 95 | Train loss: 127.7420 | Test loss: 129.0832\n","Epoch: 96 | Train loss: 127.6920 | Test loss: 128.9853\n","Epoch: 97 | Train loss: 127.6092 | Test loss: 128.9965\n","Epoch: 98 | Train loss: 127.4213 | Test loss: 128.7944\n","Epoch: 99 | Train loss: 127.3088 | Test loss: 128.7941\n","Epoch: 100 | Train loss: 127.2220 | Test loss: 128.5761\n","Epoch: 101 | Train loss: 127.0517 | Test loss: 128.3679\n","Epoch: 102 | Train loss: 126.7631 | Test loss: 128.1411\n","Epoch: 103 | Train loss: 126.5030 | Test loss: 127.7845\n","Epoch: 104 | Train loss: 126.2472 | Test loss: 127.5362\n","Epoch: 105 | Train loss: 126.1030 | Test loss: 127.4104\n","Epoch: 106 | Train loss: 125.8886 | Test loss: 127.2515\n","Epoch: 107 | Train loss: 125.7470 | Test loss: 127.0400\n","Epoch: 108 | Train loss: 125.5220 | Test loss: 126.8781\n","Epoch: 109 | Train loss: 125.2882 | Test loss: 126.5213\n","Epoch: 110 | Train loss: 124.9858 | Test loss: 126.1467\n","Epoch: 111 | Train loss: 124.5796 | Test loss: 125.8767\n","Epoch: 112 | Train loss: 124.2595 | Test loss: 125.5484\n","Epoch: 113 | Train loss: 123.9245 | Test loss: 125.1779\n","Epoch: 114 | Train loss: 123.6093 | Test loss: 124.9063\n","Epoch: 115 | Train loss: 123.3803 | Test loss: 124.9151\n","Epoch: 116 | Train loss: 123.1669 | Test loss: 124.6092\n","Epoch: 117 | Train loss: 122.9383 | Test loss: 124.4680\n","Epoch: 118 | Train loss: 122.8399 | Test loss: 124.3501\n","Epoch: 119 | Train loss: 122.6755 | Test loss: 124.1161\n","Epoch: 120 | Train loss: 122.5549 | Test loss: 123.9849\n","Epoch: 121 | Train loss: 122.4419 | Test loss: 123.9221\n","Epoch: 122 | Train loss: 122.3155 | Test loss: 123.9300\n","Epoch: 123 | Train loss: 122.2004 | Test loss: 123.6692\n","Epoch: 124 | Train loss: 122.0995 | Test loss: 123.5794\n","Epoch: 125 | Train loss: 122.0140 | Test loss: 123.6442\n","Epoch: 126 | Train loss: 121.9262 | Test loss: 123.5597\n","Epoch: 127 | Train loss: 121.8491 | Test loss: 123.3105\n","Epoch: 128 | Train loss: 121.7122 | Test loss: 123.2806\n","Epoch: 129 | Train loss: 121.6526 | Test loss: 123.2786\n","Epoch: 130 | Train loss: 121.5358 | Test loss: 123.1388\n","Epoch: 131 | Train loss: 121.4450 | Test loss: 122.9722\n","Epoch: 132 | Train loss: 121.3518 | Test loss: 122.9584\n","Epoch: 133 | Train loss: 121.2112 | Test loss: 122.8367\n","Epoch: 134 | Train loss: 121.0849 | Test loss: 122.5016\n","Epoch: 135 | Train loss: 120.8355 | Test loss: 122.3653\n","Epoch: 136 | Train loss: 120.5621 | Test loss: 122.1450\n","Epoch: 137 | Train loss: 120.4158 | Test loss: 121.9024\n","Epoch: 138 | Train loss: 120.2220 | Test loss: 121.7811\n","Epoch: 139 | Train loss: 120.0865 | Test loss: 121.7967\n","Epoch: 140 | Train loss: 120.0669 | Test loss: 121.6549\n","Epoch: 141 | Train loss: 119.9899 | Test loss: 121.4593\n","Epoch: 142 | Train loss: 119.8726 | Test loss: 121.4226\n","Epoch: 143 | Train loss: 119.7819 | Test loss: 121.2328\n","Epoch: 144 | Train loss: 119.7002 | Test loss: 121.2575\n","Epoch: 145 | Train loss: 119.5597 | Test loss: 121.2393\n","Epoch: 146 | Train loss: 119.5396 | Test loss: 121.1929\n","Epoch: 147 | Train loss: 119.4768 | Test loss: 121.0599\n","Epoch: 148 | Train loss: 119.4213 | Test loss: 121.1366\n","Epoch: 149 | Train loss: 119.3312 | Test loss: 121.0161\n","Epoch: 150 | Train loss: 119.3342 | Test loss: 121.0362\n","Epoch: 151 | Train loss: 119.2336 | Test loss: 120.8857\n","Epoch: 152 | Train loss: 119.2167 | Test loss: 120.7717\n","Epoch: 153 | Train loss: 119.1407 | Test loss: 120.8582\n","Epoch: 154 | Train loss: 119.0551 | Test loss: 120.7860\n","Epoch: 155 | Train loss: 119.0499 | Test loss: 121.1625\n","Epoch: 156 | Train loss: 118.9922 | Test loss: 120.7050\n","Epoch: 157 | Train loss: 118.9669 | Test loss: 120.8332\n","Epoch: 158 | Train loss: 118.9279 | Test loss: 120.5451\n","Epoch: 159 | Train loss: 118.8751 | Test loss: 120.5052\n","Epoch: 160 | Train loss: 118.8411 | Test loss: 120.5638\n","Epoch: 161 | Train loss: 118.7812 | Test loss: 120.7024\n","Epoch: 162 | Train loss: 118.7277 | Test loss: 120.4474\n","Epoch: 163 | Train loss: 118.7033 | Test loss: 120.4557\n","Epoch: 164 | Train loss: 118.6478 | Test loss: 120.4489\n","Epoch: 165 | Train loss: 118.6539 | Test loss: 120.3164\n","Epoch: 166 | Train loss: 118.5753 | Test loss: 120.3208\n","Epoch: 167 | Train loss: 118.5711 | Test loss: 120.2742\n","Epoch: 168 | Train loss: 118.5130 | Test loss: 120.2464\n","Epoch: 169 | Train loss: 118.4619 | Test loss: 120.6972\n","Epoch: 170 | Train loss: 118.4063 | Test loss: 120.4721\n","Epoch: 171 | Train loss: 118.3442 | Test loss: 120.0948\n","Epoch: 172 | Train loss: 118.3266 | Test loss: 120.0483\n","Epoch: 173 | Train loss: 118.1718 | Test loss: 120.2640\n","Epoch: 174 | Train loss: 118.1764 | Test loss: 119.7952\n","Epoch: 175 | Train loss: 118.0554 | Test loss: 119.7689\n","Epoch: 176 | Train loss: 117.9376 | Test loss: 119.7067\n","Epoch: 177 | Train loss: 117.7756 | Test loss: 119.6330\n","Epoch: 178 | Train loss: 117.7095 | Test loss: 119.3623\n","Epoch: 179 | Train loss: 117.6502 | Test loss: 120.2700\n","Epoch: 180 | Train loss: 117.5058 | Test loss: 119.2453\n","Epoch: 181 | Train loss: 117.3982 | Test loss: 119.0273\n","Epoch: 182 | Train loss: 117.3248 | Test loss: 118.9876\n","Epoch: 183 | Train loss: 117.2547 | Test loss: 118.7941\n","Epoch: 184 | Train loss: 117.1939 | Test loss: 118.8688\n","Epoch: 185 | Train loss: 117.0985 | Test loss: 118.8808\n","Epoch: 186 | Train loss: 117.1049 | Test loss: 118.7367\n","Epoch: 187 | Train loss: 116.9774 | Test loss: 118.7150\n","Epoch: 188 | Train loss: 116.8632 | Test loss: 118.6564\n","Epoch: 189 | Train loss: 116.9180 | Test loss: 118.5342\n","Epoch: 190 | Train loss: 116.7558 | Test loss: 119.1934\n","Epoch: 191 | Train loss: 116.8267 | Test loss: 118.4838\n","Epoch: 192 | Train loss: 116.6884 | Test loss: 118.3289\n","Epoch: 193 | Train loss: 116.6061 | Test loss: 118.2645\n","Epoch: 194 | Train loss: 116.4648 | Test loss: 118.2913\n","Epoch: 195 | Train loss: 116.4767 | Test loss: 118.3303\n","Epoch: 196 | Train loss: 116.2985 | Test loss: 118.2506\n","Epoch: 197 | Train loss: 116.2125 | Test loss: 117.9484\n","Epoch: 198 | Train loss: 116.0481 | Test loss: 118.4568\n","Epoch: 199 | Train loss: 115.9107 | Test loss: 117.6879\n","Epoch: 200 | Train loss: 115.8623 | Test loss: 117.6089\n","Epoch: 201 | Train loss: 115.8072 | Test loss: 117.9811\n","Epoch: 202 | Train loss: 115.7137 | Test loss: 117.6354\n","Epoch: 203 | Train loss: 115.5778 | Test loss: 117.6884\n","Epoch: 204 | Train loss: 115.5256 | Test loss: 117.4420\n","Epoch: 205 | Train loss: 115.5069 | Test loss: 117.2000\n","Epoch: 206 | Train loss: 115.4658 | Test loss: 117.3069\n","Epoch: 207 | Train loss: 115.4761 | Test loss: 117.3002\n","Epoch: 208 | Train loss: 115.3259 | Test loss: 117.3324\n","Epoch: 209 | Train loss: 115.2675 | Test loss: 117.2204\n","Epoch: 210 | Train loss: 115.2938 | Test loss: 117.2627\n","Epoch: 211 | Train loss: 115.1970 | Test loss: 116.9823\n","Epoch: 212 | Train loss: 115.2205 | Test loss: 117.1155\n","Epoch: 213 | Train loss: 115.0987 | Test loss: 117.3661\n","Epoch: 214 | Train loss: 115.1466 | Test loss: 117.6428\n","Epoch: 215 | Train loss: 115.1881 | Test loss: 116.8514\n","Epoch: 216 | Train loss: 115.0613 | Test loss: 117.0708\n","Epoch: 217 | Train loss: 114.9700 | Test loss: 117.1031\n","Epoch: 218 | Train loss: 115.0101 | Test loss: 116.8489\n","Epoch: 219 | Train loss: 114.9195 | Test loss: 116.8170\n","Epoch: 220 | Train loss: 114.8624 | Test loss: 116.8977\n","Epoch: 221 | Train loss: 114.8162 | Test loss: 116.7710\n","Epoch: 222 | Train loss: 114.7945 | Test loss: 116.6308\n","Epoch: 223 | Train loss: 114.7770 | Test loss: 116.5714\n","Epoch: 224 | Train loss: 114.7150 | Test loss: 116.5068\n","Epoch: 225 | Train loss: 114.6803 | Test loss: 116.9699\n","Epoch: 226 | Train loss: 114.6858 | Test loss: 116.6521\n","Epoch: 227 | Train loss: 114.6704 | Test loss: 116.5213\n","Epoch: 228 | Train loss: 114.6049 | Test loss: 116.5065\n","Epoch: 229 | Train loss: 114.5642 | Test loss: 116.4949\n","Epoch: 230 | Train loss: 114.5963 | Test loss: 116.5036\n","Epoch: 231 | Train loss: 114.4725 | Test loss: 116.4700\n","Epoch: 232 | Train loss: 114.5235 | Test loss: 116.3065\n","Epoch: 233 | Train loss: 114.4778 | Test loss: 116.3553\n","Epoch: 234 | Train loss: 114.4481 | Test loss: 116.5006\n","Epoch: 235 | Train loss: 114.4555 | Test loss: 116.1840\n","Epoch: 236 | Train loss: 114.4099 | Test loss: 116.2818\n","Epoch: 237 | Train loss: 114.3154 | Test loss: 116.1683\n","Epoch: 238 | Train loss: 114.2926 | Test loss: 116.2070\n","Epoch: 239 | Train loss: 114.2539 | Test loss: 116.0726\n","Epoch: 240 | Train loss: 114.2762 | Test loss: 116.2075\n","Epoch: 241 | Train loss: 114.1817 | Test loss: 116.1279\n","Epoch: 242 | Train loss: 114.2383 | Test loss: 116.0622\n","Epoch: 243 | Train loss: 114.1501 | Test loss: 115.9429\n","Epoch: 244 | Train loss: 114.1136 | Test loss: 115.8173\n","Epoch: 245 | Train loss: 114.0476 | Test loss: 115.7521\n","Epoch: 246 | Train loss: 113.7857 | Test loss: 115.7396\n","Epoch: 247 | Train loss: 113.6603 | Test loss: 115.4084\n","Epoch: 248 | Train loss: 113.4537 | Test loss: 115.4087\n","Epoch: 249 | Train loss: 113.2842 | Test loss: 115.1063\n","Epoch: 250 | Train loss: 113.1966 | Test loss: 115.1302\n","Epoch: 251 | Train loss: 113.1398 | Test loss: 114.9744\n","Epoch: 252 | Train loss: 113.0508 | Test loss: 114.9586\n","Epoch: 253 | Train loss: 112.9532 | Test loss: 115.0364\n","Epoch: 254 | Train loss: 112.9721 | Test loss: 114.8502\n","Epoch: 255 | Train loss: 112.9396 | Test loss: 114.7185\n","Epoch: 256 | Train loss: 112.8943 | Test loss: 114.5962\n","Epoch: 257 | Train loss: 112.7965 | Test loss: 114.7022\n","Epoch: 258 | Train loss: 112.7687 | Test loss: 114.6617\n","Epoch: 259 | Train loss: 112.7154 | Test loss: 114.8442\n","Epoch: 260 | Train loss: 112.7020 | Test loss: 114.4959\n","Epoch: 261 | Train loss: 112.6818 | Test loss: 114.5980\n","Epoch: 262 | Train loss: 112.6464 | Test loss: 114.4955\n","Epoch: 263 | Train loss: 112.5994 | Test loss: 114.5252\n","Epoch: 264 | Train loss: 112.5926 | Test loss: 114.4325\n","Epoch: 265 | Train loss: 112.5506 | Test loss: 114.6705\n","Epoch: 266 | Train loss: 112.4769 | Test loss: 114.4225\n","Epoch: 267 | Train loss: 112.4829 | Test loss: 114.3169\n","Epoch: 268 | Train loss: 112.4421 | Test loss: 114.6645\n","Epoch: 269 | Train loss: 112.4229 | Test loss: 114.2725\n","Epoch: 270 | Train loss: 112.3688 | Test loss: 114.7447\n","Epoch: 271 | Train loss: 112.2765 | Test loss: 114.1662\n","Epoch: 272 | Train loss: 112.1758 | Test loss: 114.2483\n","Epoch: 273 | Train loss: 112.1149 | Test loss: 114.1094\n","Epoch: 274 | Train loss: 112.0413 | Test loss: 114.1148\n","Epoch: 275 | Train loss: 111.9061 | Test loss: 113.6663\n","Epoch: 276 | Train loss: 111.8440 | Test loss: 113.7327\n","Epoch: 277 | Train loss: 111.7298 | Test loss: 113.5811\n","Epoch: 278 | Train loss: 111.6706 | Test loss: 113.7182\n","Epoch: 279 | Train loss: 111.6635 | Test loss: 113.5574\n","Epoch: 280 | Train loss: 111.5622 | Test loss: 113.4522\n","Epoch: 281 | Train loss: 111.5728 | Test loss: 113.3696\n","Epoch: 282 | Train loss: 111.6261 | Test loss: 113.3308\n","Epoch: 283 | Train loss: 111.5232 | Test loss: 113.3728\n","Epoch: 284 | Train loss: 111.5053 | Test loss: 113.5639\n","Epoch: 285 | Train loss: 111.4669 | Test loss: 113.5118\n","Epoch: 286 | Train loss: 111.5149 | Test loss: 113.2694\n","Epoch: 287 | Train loss: 111.4563 | Test loss: 113.3170\n","Epoch: 288 | Train loss: 111.4739 | Test loss: 113.2406\n","Epoch: 289 | Train loss: 111.3584 | Test loss: 113.5422\n","Epoch: 290 | Train loss: 111.4710 | Test loss: 113.3026\n","Epoch: 291 | Train loss: 111.4088 | Test loss: 113.3519\n","Epoch: 292 | Train loss: 111.3549 | Test loss: 113.2689\n","Epoch: 293 | Train loss: 111.4308 | Test loss: 113.2945\n","Epoch: 294 | Train loss: 111.3597 | Test loss: 113.7529\n","Epoch: 295 | Train loss: 111.3076 | Test loss: 113.3307\n","Epoch: 296 | Train loss: 111.3430 | Test loss: 113.3755\n","Epoch: 297 | Train loss: 111.3504 | Test loss: 113.3405\n","Epoch: 298 | Train loss: 111.3032 | Test loss: 113.3442\n","Epoch: 299 | Train loss: 111.3354 | Test loss: 113.4090\n","Epoch: 300 | Train loss: 111.2793 | Test loss: 113.2329\n","Epoch: 301 | Train loss: 111.2949 | Test loss: 113.3532\n","Epoch: 302 | Train loss: 111.2258 | Test loss: 113.3418\n","Epoch: 303 | Train loss: 111.3266 | Test loss: 113.2116\n","Epoch: 304 | Train loss: 111.2167 | Test loss: 113.1641\n","Epoch: 305 | Train loss: 111.2156 | Test loss: 113.2364\n","Epoch: 306 | Train loss: 111.2242 | Test loss: 113.0671\n","Epoch: 307 | Train loss: 111.2484 | Test loss: 113.1584\n","Epoch: 308 | Train loss: 111.1932 | Test loss: 113.0949\n","Epoch: 309 | Train loss: 111.1329 | Test loss: 113.0478\n","Epoch: 310 | Train loss: 111.1885 | Test loss: 113.0158\n","Epoch: 311 | Train loss: 111.1672 | Test loss: 113.2409\n","Epoch: 312 | Train loss: 111.1354 | Test loss: 113.1102\n","Epoch: 313 | Train loss: 111.1330 | Test loss: 112.9745\n","Epoch: 314 | Train loss: 111.0824 | Test loss: 113.0523\n","Epoch: 315 | Train loss: 111.1257 | Test loss: 113.0562\n","Epoch: 316 | Train loss: 111.0945 | Test loss: 113.1314\n","Epoch: 317 | Train loss: 111.0797 | Test loss: 113.9774\n","Epoch: 318 | Train loss: 111.1074 | Test loss: 113.4828\n","Epoch: 319 | Train loss: 111.0878 | Test loss: 113.0456\n","Epoch: 320 | Train loss: 111.0213 | Test loss: 113.1914\n","Epoch: 321 | Train loss: 111.0453 | Test loss: 113.0299\n","Epoch: 322 | Train loss: 111.0170 | Test loss: 112.8828\n","Epoch: 323 | Train loss: 110.9929 | Test loss: 112.9474\n","Epoch: 324 | Train loss: 111.0001 | Test loss: 112.9318\n","Epoch: 325 | Train loss: 110.9574 | Test loss: 113.0025\n","Epoch: 326 | Train loss: 110.9451 | Test loss: 113.0282\n","Epoch: 327 | Train loss: 110.9257 | Test loss: 113.0421\n","Epoch: 328 | Train loss: 110.9348 | Test loss: 112.8912\n","Epoch: 329 | Train loss: 110.9417 | Test loss: 112.8911\n","Epoch: 330 | Train loss: 110.9039 | Test loss: 112.8012\n","Epoch: 331 | Train loss: 110.8750 | Test loss: 113.0927\n","Epoch: 332 | Train loss: 110.8345 | Test loss: 112.8919\n","Epoch: 333 | Train loss: 110.8790 | Test loss: 112.9118\n","Epoch: 334 | Train loss: 110.8796 | Test loss: 112.7332\n","Epoch: 335 | Train loss: 110.8132 | Test loss: 112.7626\n","Epoch: 336 | Train loss: 110.8217 | Test loss: 112.8502\n","Epoch: 337 | Train loss: 110.8368 | Test loss: 112.6867\n","Epoch: 338 | Train loss: 110.7764 | Test loss: 112.7482\n","Epoch: 339 | Train loss: 110.7758 | Test loss: 112.9129\n","Epoch: 340 | Train loss: 110.7398 | Test loss: 112.5984\n","Epoch: 341 | Train loss: 110.6651 | Test loss: 112.6716\n","Epoch: 342 | Train loss: 110.7399 | Test loss: 113.0462\n","Epoch: 343 | Train loss: 110.6900 | Test loss: 112.6022\n","Epoch: 344 | Train loss: 110.6910 | Test loss: 112.7151\n","Epoch: 345 | Train loss: 110.7485 | Test loss: 112.6074\n","Epoch: 346 | Train loss: 110.5963 | Test loss: 112.6263\n","Epoch: 347 | Train loss: 110.6285 | Test loss: 112.8282\n","Epoch: 348 | Train loss: 110.6171 | Test loss: 112.5807\n","Epoch: 349 | Train loss: 110.6022 | Test loss: 112.9360\n","Epoch: 350 | Train loss: 110.6161 | Test loss: 112.5394\n","Epoch: 351 | Train loss: 110.5684 | Test loss: 112.4978\n","Epoch: 352 | Train loss: 110.5864 | Test loss: 112.5232\n","Epoch: 353 | Train loss: 110.5664 | Test loss: 112.6702\n","Epoch: 354 | Train loss: 110.5554 | Test loss: 112.4504\n","Epoch: 355 | Train loss: 110.5089 | Test loss: 112.5339\n","Epoch: 356 | Train loss: 110.5387 | Test loss: 112.4862\n","Epoch: 357 | Train loss: 110.5666 | Test loss: 112.6446\n","Epoch: 358 | Train loss: 110.5293 | Test loss: 112.4659\n","Epoch: 359 | Train loss: 110.4901 | Test loss: 113.0786\n","Epoch: 360 | Train loss: 110.5087 | Test loss: 112.5338\n","Epoch: 361 | Train loss: 110.4289 | Test loss: 112.4027\n","Epoch: 362 | Train loss: 110.4428 | Test loss: 112.6276\n","Epoch: 363 | Train loss: 110.4590 | Test loss: 112.3917\n","Epoch: 364 | Train loss: 110.3851 | Test loss: 112.3912\n","Epoch: 365 | Train loss: 110.4491 | Test loss: 112.3260\n","Epoch: 366 | Train loss: 110.4071 | Test loss: 112.3699\n","Epoch: 367 | Train loss: 110.4044 | Test loss: 112.4220\n","Epoch: 368 | Train loss: 110.3886 | Test loss: 112.3572\n","Epoch: 369 | Train loss: 110.3537 | Test loss: 112.3582\n","Epoch: 370 | Train loss: 110.3729 | Test loss: 112.2910\n","Epoch: 371 | Train loss: 110.4027 | Test loss: 112.3228\n","Epoch: 372 | Train loss: 110.4226 | Test loss: 112.3017\n","Epoch: 373 | Train loss: 110.4071 | Test loss: 112.3978\n","Epoch: 374 | Train loss: 110.3446 | Test loss: 112.3521\n","Epoch: 375 | Train loss: 110.3462 | Test loss: 112.4277\n","Epoch: 376 | Train loss: 110.3743 | Test loss: 112.4660\n","Epoch: 377 | Train loss: 110.3047 | Test loss: 112.3454\n","Epoch: 378 | Train loss: 110.3787 | Test loss: 112.4426\n","Epoch: 379 | Train loss: 110.3594 | Test loss: 112.3238\n","Epoch: 380 | Train loss: 110.3207 | Test loss: 112.2427\n","Epoch: 381 | Train loss: 110.2883 | Test loss: 112.3267\n","Epoch: 382 | Train loss: 110.3104 | Test loss: 112.2499\n","Epoch: 383 | Train loss: 110.3020 | Test loss: 112.3793\n","Epoch: 384 | Train loss: 110.3148 | Test loss: 112.3066\n","Epoch: 385 | Train loss: 110.3530 | Test loss: 112.5555\n","Epoch: 386 | Train loss: 110.3232 | Test loss: 112.3765\n","Epoch: 387 | Train loss: 110.3216 | Test loss: 112.7890\n","Epoch: 388 | Train loss: 110.2943 | Test loss: 112.4320\n","Epoch: 389 | Train loss: 110.2895 | Test loss: 112.6812\n","Epoch: 390 | Train loss: 110.2907 | Test loss: 112.1710\n","Epoch: 391 | Train loss: 110.2914 | Test loss: 112.3477\n","Epoch: 392 | Train loss: 110.2909 | Test loss: 112.2224\n","Epoch: 393 | Train loss: 110.2367 | Test loss: 112.1537\n","Epoch: 394 | Train loss: 110.2819 | Test loss: 112.2835\n","Epoch: 395 | Train loss: 110.2429 | Test loss: 112.2210\n","Epoch: 396 | Train loss: 110.2586 | Test loss: 112.1344\n","Epoch: 397 | Train loss: 110.2949 | Test loss: 112.1523\n","Epoch: 398 | Train loss: 110.2341 | Test loss: 112.2118\n","Epoch: 399 | Train loss: 110.2268 | Test loss: 112.2493\n","Total training time: 930.297 seconds\n"]}]}]}